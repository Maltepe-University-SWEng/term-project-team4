{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 2834,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007057163020465773,
      "grad_norm": 9.803939819335938,
      "learning_rate": 4.989414255469302e-05,
      "loss": 5.7008,
      "step": 10
    },
    {
      "epoch": 0.014114326040931546,
      "grad_norm": 14.286739349365234,
      "learning_rate": 4.9776523171018584e-05,
      "loss": 5.59,
      "step": 20
    },
    {
      "epoch": 0.02117148906139732,
      "grad_norm": 9.54956340789795,
      "learning_rate": 4.9658903787344156e-05,
      "loss": 5.2107,
      "step": 30
    },
    {
      "epoch": 0.028228652081863093,
      "grad_norm": 9.296425819396973,
      "learning_rate": 4.954128440366973e-05,
      "loss": 5.1773,
      "step": 40
    },
    {
      "epoch": 0.035285815102328866,
      "grad_norm": 9.932609558105469,
      "learning_rate": 4.9423665019995294e-05,
      "loss": 5.1755,
      "step": 50
    },
    {
      "epoch": 0.04234297812279464,
      "grad_norm": 9.96810245513916,
      "learning_rate": 4.9306045636320866e-05,
      "loss": 5.0035,
      "step": 60
    },
    {
      "epoch": 0.04940014114326041,
      "grad_norm": 9.972799301147461,
      "learning_rate": 4.9188426252646444e-05,
      "loss": 5.345,
      "step": 70
    },
    {
      "epoch": 0.056457304163726185,
      "grad_norm": 12.471413612365723,
      "learning_rate": 4.907080686897201e-05,
      "loss": 5.0411,
      "step": 80
    },
    {
      "epoch": 0.06351446718419196,
      "grad_norm": 9.28343677520752,
      "learning_rate": 4.895318748529758e-05,
      "loss": 5.1438,
      "step": 90
    },
    {
      "epoch": 0.07057163020465773,
      "grad_norm": 9.345253944396973,
      "learning_rate": 4.8835568101623154e-05,
      "loss": 4.9463,
      "step": 100
    },
    {
      "epoch": 0.0776287932251235,
      "grad_norm": 9.808815956115723,
      "learning_rate": 4.871794871794872e-05,
      "loss": 5.1154,
      "step": 110
    },
    {
      "epoch": 0.08468595624558928,
      "grad_norm": 9.499787330627441,
      "learning_rate": 4.860032933427429e-05,
      "loss": 4.9691,
      "step": 120
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 10.887115478515625,
      "learning_rate": 4.848270995059986e-05,
      "loss": 5.1977,
      "step": 130
    },
    {
      "epoch": 0.09880028228652082,
      "grad_norm": 9.585956573486328,
      "learning_rate": 4.836509056692543e-05,
      "loss": 4.8795,
      "step": 140
    },
    {
      "epoch": 0.1058574453069866,
      "grad_norm": 10.024253845214844,
      "learning_rate": 4.8247471183251e-05,
      "loss": 5.0115,
      "step": 150
    },
    {
      "epoch": 0.11291460832745237,
      "grad_norm": 10.05254077911377,
      "learning_rate": 4.812985179957657e-05,
      "loss": 5.0347,
      "step": 160
    },
    {
      "epoch": 0.11997177134791814,
      "grad_norm": 9.168549537658691,
      "learning_rate": 4.8012232415902144e-05,
      "loss": 5.2137,
      "step": 170
    },
    {
      "epoch": 0.12702893436838392,
      "grad_norm": 10.453447341918945,
      "learning_rate": 4.7894613032227716e-05,
      "loss": 5.047,
      "step": 180
    },
    {
      "epoch": 0.1340860973888497,
      "grad_norm": 8.862540245056152,
      "learning_rate": 4.777699364855329e-05,
      "loss": 5.0782,
      "step": 190
    },
    {
      "epoch": 0.14114326040931546,
      "grad_norm": 8.496728897094727,
      "learning_rate": 4.765937426487885e-05,
      "loss": 4.7925,
      "step": 200
    },
    {
      "epoch": 0.14820042342978124,
      "grad_norm": 9.522027969360352,
      "learning_rate": 4.7541754881204425e-05,
      "loss": 4.5334,
      "step": 210
    },
    {
      "epoch": 0.155257586450247,
      "grad_norm": 10.281990051269531,
      "learning_rate": 4.742413549753e-05,
      "loss": 4.8138,
      "step": 220
    },
    {
      "epoch": 0.16231474947071278,
      "grad_norm": 10.3073148727417,
      "learning_rate": 4.730651611385556e-05,
      "loss": 4.8046,
      "step": 230
    },
    {
      "epoch": 0.16937191249117856,
      "grad_norm": 9.511117935180664,
      "learning_rate": 4.7188896730181134e-05,
      "loss": 5.0472,
      "step": 240
    },
    {
      "epoch": 0.17642907551164433,
      "grad_norm": 9.618687629699707,
      "learning_rate": 4.7071277346506706e-05,
      "loss": 4.8456,
      "step": 250
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 8.858317375183105,
      "learning_rate": 4.695365796283228e-05,
      "loss": 4.8941,
      "step": 260
    },
    {
      "epoch": 0.19054340155257588,
      "grad_norm": 10.215332984924316,
      "learning_rate": 4.683603857915785e-05,
      "loss": 4.801,
      "step": 270
    },
    {
      "epoch": 0.19760056457304165,
      "grad_norm": 9.842853546142578,
      "learning_rate": 4.671841919548342e-05,
      "loss": 4.8424,
      "step": 280
    },
    {
      "epoch": 0.20465772759350742,
      "grad_norm": 7.9486918449401855,
      "learning_rate": 4.660079981180899e-05,
      "loss": 4.6034,
      "step": 290
    },
    {
      "epoch": 0.2117148906139732,
      "grad_norm": 10.1187744140625,
      "learning_rate": 4.648318042813456e-05,
      "loss": 4.6096,
      "step": 300
    },
    {
      "epoch": 0.21877205363443897,
      "grad_norm": 9.774480819702148,
      "learning_rate": 4.636556104446013e-05,
      "loss": 4.8018,
      "step": 310
    },
    {
      "epoch": 0.22582921665490474,
      "grad_norm": 9.91816520690918,
      "learning_rate": 4.6247941660785696e-05,
      "loss": 4.7862,
      "step": 320
    },
    {
      "epoch": 0.23288637967537051,
      "grad_norm": 9.578490257263184,
      "learning_rate": 4.613032227711127e-05,
      "loss": 4.9187,
      "step": 330
    },
    {
      "epoch": 0.2399435426958363,
      "grad_norm": 9.230071067810059,
      "learning_rate": 4.601270289343684e-05,
      "loss": 4.6422,
      "step": 340
    },
    {
      "epoch": 0.24700070571630206,
      "grad_norm": 10.706733703613281,
      "learning_rate": 4.589508350976241e-05,
      "loss": 4.7011,
      "step": 350
    },
    {
      "epoch": 0.25405786873676783,
      "grad_norm": 8.942960739135742,
      "learning_rate": 4.5777464126087984e-05,
      "loss": 4.9398,
      "step": 360
    },
    {
      "epoch": 0.2611150317572336,
      "grad_norm": 9.923922538757324,
      "learning_rate": 4.5659844742413556e-05,
      "loss": 5.1796,
      "step": 370
    },
    {
      "epoch": 0.2681721947776994,
      "grad_norm": 9.513066291809082,
      "learning_rate": 4.554222535873912e-05,
      "loss": 5.056,
      "step": 380
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 9.730828285217285,
      "learning_rate": 4.5424605975064694e-05,
      "loss": 5.0605,
      "step": 390
    },
    {
      "epoch": 0.2822865208186309,
      "grad_norm": 9.58809757232666,
      "learning_rate": 4.5306986591390266e-05,
      "loss": 5.0821,
      "step": 400
    },
    {
      "epoch": 0.2893436838390967,
      "grad_norm": 9.873540878295898,
      "learning_rate": 4.518936720771583e-05,
      "loss": 5.0323,
      "step": 410
    },
    {
      "epoch": 0.2964008468595625,
      "grad_norm": 9.330329895019531,
      "learning_rate": 4.50717478240414e-05,
      "loss": 4.5335,
      "step": 420
    },
    {
      "epoch": 0.30345800988002825,
      "grad_norm": 10.289673805236816,
      "learning_rate": 4.4954128440366975e-05,
      "loss": 4.9622,
      "step": 430
    },
    {
      "epoch": 0.310515172900494,
      "grad_norm": 9.374435424804688,
      "learning_rate": 4.483650905669255e-05,
      "loss": 4.788,
      "step": 440
    },
    {
      "epoch": 0.3175723359209598,
      "grad_norm": 9.762895584106445,
      "learning_rate": 4.471888967301812e-05,
      "loss": 4.9906,
      "step": 450
    },
    {
      "epoch": 0.32462949894142556,
      "grad_norm": 11.105172157287598,
      "learning_rate": 4.460127028934369e-05,
      "loss": 4.7867,
      "step": 460
    },
    {
      "epoch": 0.33168666196189134,
      "grad_norm": 9.769625663757324,
      "learning_rate": 4.4483650905669256e-05,
      "loss": 4.6602,
      "step": 470
    },
    {
      "epoch": 0.3387438249823571,
      "grad_norm": 9.89722728729248,
      "learning_rate": 4.436603152199483e-05,
      "loss": 4.5194,
      "step": 480
    },
    {
      "epoch": 0.3458009880028229,
      "grad_norm": 8.95580768585205,
      "learning_rate": 4.42484121383204e-05,
      "loss": 4.5984,
      "step": 490
    },
    {
      "epoch": 0.35285815102328866,
      "grad_norm": 9.174824714660645,
      "learning_rate": 4.4130792754645965e-05,
      "loss": 4.7109,
      "step": 500
    },
    {
      "epoch": 0.35991531404375443,
      "grad_norm": 10.257124900817871,
      "learning_rate": 4.401317337097154e-05,
      "loss": 4.7568,
      "step": 510
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 9.179488182067871,
      "learning_rate": 4.389555398729711e-05,
      "loss": 4.6118,
      "step": 520
    },
    {
      "epoch": 0.374029640084686,
      "grad_norm": 9.112079620361328,
      "learning_rate": 4.377793460362268e-05,
      "loss": 5.1196,
      "step": 530
    },
    {
      "epoch": 0.38108680310515175,
      "grad_norm": 9.738810539245605,
      "learning_rate": 4.366031521994825e-05,
      "loss": 4.9859,
      "step": 540
    },
    {
      "epoch": 0.3881439661256175,
      "grad_norm": 10.382537841796875,
      "learning_rate": 4.3542695836273825e-05,
      "loss": 4.7305,
      "step": 550
    },
    {
      "epoch": 0.3952011291460833,
      "grad_norm": 9.557065963745117,
      "learning_rate": 4.342507645259939e-05,
      "loss": 4.7168,
      "step": 560
    },
    {
      "epoch": 0.40225829216654907,
      "grad_norm": 10.748839378356934,
      "learning_rate": 4.330745706892496e-05,
      "loss": 4.5533,
      "step": 570
    },
    {
      "epoch": 0.40931545518701484,
      "grad_norm": 10.571599006652832,
      "learning_rate": 4.3189837685250534e-05,
      "loss": 4.9693,
      "step": 580
    },
    {
      "epoch": 0.4163726182074806,
      "grad_norm": 13.34694766998291,
      "learning_rate": 4.30722183015761e-05,
      "loss": 5.1653,
      "step": 590
    },
    {
      "epoch": 0.4234297812279464,
      "grad_norm": 8.879600524902344,
      "learning_rate": 4.295459891790167e-05,
      "loss": 4.7256,
      "step": 600
    },
    {
      "epoch": 0.43048694424841216,
      "grad_norm": 11.075096130371094,
      "learning_rate": 4.283697953422724e-05,
      "loss": 4.7512,
      "step": 610
    },
    {
      "epoch": 0.43754410726887794,
      "grad_norm": 10.101607322692871,
      "learning_rate": 4.271936015055281e-05,
      "loss": 4.5086,
      "step": 620
    },
    {
      "epoch": 0.4446012702893437,
      "grad_norm": 9.3810453414917,
      "learning_rate": 4.260174076687839e-05,
      "loss": 5.0921,
      "step": 630
    },
    {
      "epoch": 0.4516584333098095,
      "grad_norm": 9.491238594055176,
      "learning_rate": 4.248412138320396e-05,
      "loss": 4.5439,
      "step": 640
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 8.572540283203125,
      "learning_rate": 4.2366501999529525e-05,
      "loss": 4.7219,
      "step": 650
    },
    {
      "epoch": 0.46577275935074103,
      "grad_norm": 9.594265937805176,
      "learning_rate": 4.2248882615855096e-05,
      "loss": 4.1834,
      "step": 660
    },
    {
      "epoch": 0.4728299223712068,
      "grad_norm": 9.219919204711914,
      "learning_rate": 4.213126323218067e-05,
      "loss": 4.6037,
      "step": 670
    },
    {
      "epoch": 0.4798870853916726,
      "grad_norm": 9.437829971313477,
      "learning_rate": 4.2013643848506234e-05,
      "loss": 4.6404,
      "step": 680
    },
    {
      "epoch": 0.48694424841213835,
      "grad_norm": 9.40666389465332,
      "learning_rate": 4.1896024464831806e-05,
      "loss": 4.3007,
      "step": 690
    },
    {
      "epoch": 0.4940014114326041,
      "grad_norm": 8.271156311035156,
      "learning_rate": 4.177840508115738e-05,
      "loss": 4.985,
      "step": 700
    },
    {
      "epoch": 0.5010585744530699,
      "grad_norm": 9.671629905700684,
      "learning_rate": 4.166078569748294e-05,
      "loss": 4.6099,
      "step": 710
    },
    {
      "epoch": 0.5081157374735357,
      "grad_norm": 9.2694673538208,
      "learning_rate": 4.1543166313808515e-05,
      "loss": 4.8777,
      "step": 720
    },
    {
      "epoch": 0.5151729004940014,
      "grad_norm": 8.41503620147705,
      "learning_rate": 4.1425546930134094e-05,
      "loss": 4.5038,
      "step": 730
    },
    {
      "epoch": 0.5222300635144672,
      "grad_norm": 9.816900253295898,
      "learning_rate": 4.130792754645966e-05,
      "loss": 4.8163,
      "step": 740
    },
    {
      "epoch": 0.529287226534933,
      "grad_norm": 8.9749755859375,
      "learning_rate": 4.119030816278523e-05,
      "loss": 4.2993,
      "step": 750
    },
    {
      "epoch": 0.5363443895553988,
      "grad_norm": 10.370122909545898,
      "learning_rate": 4.10726887791108e-05,
      "loss": 4.8318,
      "step": 760
    },
    {
      "epoch": 0.5434015525758645,
      "grad_norm": 9.695919036865234,
      "learning_rate": 4.095506939543637e-05,
      "loss": 4.9918,
      "step": 770
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 9.364436149597168,
      "learning_rate": 4.083745001176194e-05,
      "loss": 4.6451,
      "step": 780
    },
    {
      "epoch": 0.5575158786167961,
      "grad_norm": 8.979764938354492,
      "learning_rate": 4.071983062808751e-05,
      "loss": 4.5565,
      "step": 790
    },
    {
      "epoch": 0.5645730416372619,
      "grad_norm": 9.374395370483398,
      "learning_rate": 4.060221124441308e-05,
      "loss": 4.6672,
      "step": 800
    },
    {
      "epoch": 0.5716302046577276,
      "grad_norm": 10.20457935333252,
      "learning_rate": 4.048459186073865e-05,
      "loss": 4.638,
      "step": 810
    },
    {
      "epoch": 0.5786873676781934,
      "grad_norm": 8.38465404510498,
      "learning_rate": 4.036697247706422e-05,
      "loss": 4.6902,
      "step": 820
    },
    {
      "epoch": 0.5857445306986592,
      "grad_norm": 10.00658130645752,
      "learning_rate": 4.024935309338979e-05,
      "loss": 4.7957,
      "step": 830
    },
    {
      "epoch": 0.592801693719125,
      "grad_norm": 9.82632827758789,
      "learning_rate": 4.0131733709715365e-05,
      "loss": 4.4547,
      "step": 840
    },
    {
      "epoch": 0.5998588567395907,
      "grad_norm": 11.756580352783203,
      "learning_rate": 4.001411432604094e-05,
      "loss": 4.9262,
      "step": 850
    },
    {
      "epoch": 0.6069160197600565,
      "grad_norm": 10.166850090026855,
      "learning_rate": 3.98964949423665e-05,
      "loss": 4.83,
      "step": 860
    },
    {
      "epoch": 0.6139731827805223,
      "grad_norm": 9.537120819091797,
      "learning_rate": 3.9778875558692074e-05,
      "loss": 5.1167,
      "step": 870
    },
    {
      "epoch": 0.621030345800988,
      "grad_norm": 8.81291389465332,
      "learning_rate": 3.9661256175017646e-05,
      "loss": 4.0962,
      "step": 880
    },
    {
      "epoch": 0.6280875088214538,
      "grad_norm": 9.698421478271484,
      "learning_rate": 3.954363679134321e-05,
      "loss": 4.7771,
      "step": 890
    },
    {
      "epoch": 0.6351446718419196,
      "grad_norm": 9.524030685424805,
      "learning_rate": 3.9426017407668783e-05,
      "loss": 4.7487,
      "step": 900
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 10.549434661865234,
      "learning_rate": 3.9308398023994355e-05,
      "loss": 4.3276,
      "step": 910
    },
    {
      "epoch": 0.6492589978828511,
      "grad_norm": 9.762537002563477,
      "learning_rate": 3.919077864031993e-05,
      "loss": 4.9132,
      "step": 920
    },
    {
      "epoch": 0.6563161609033169,
      "grad_norm": 9.407556533813477,
      "learning_rate": 3.90731592566455e-05,
      "loss": 4.4718,
      "step": 930
    },
    {
      "epoch": 0.6633733239237827,
      "grad_norm": 8.347285270690918,
      "learning_rate": 3.895553987297107e-05,
      "loss": 4.1362,
      "step": 940
    },
    {
      "epoch": 0.6704304869442484,
      "grad_norm": 9.125069618225098,
      "learning_rate": 3.8837920489296637e-05,
      "loss": 4.4408,
      "step": 950
    },
    {
      "epoch": 0.6774876499647142,
      "grad_norm": 9.320974349975586,
      "learning_rate": 3.872030110562221e-05,
      "loss": 4.6815,
      "step": 960
    },
    {
      "epoch": 0.68454481298518,
      "grad_norm": 9.82960033416748,
      "learning_rate": 3.860268172194778e-05,
      "loss": 4.8312,
      "step": 970
    },
    {
      "epoch": 0.6916019760056458,
      "grad_norm": 9.80321216583252,
      "learning_rate": 3.8485062338273346e-05,
      "loss": 4.5655,
      "step": 980
    },
    {
      "epoch": 0.6986591390261115,
      "grad_norm": 12.631752014160156,
      "learning_rate": 3.836744295459892e-05,
      "loss": 4.8426,
      "step": 990
    },
    {
      "epoch": 0.7057163020465773,
      "grad_norm": 10.28292465209961,
      "learning_rate": 3.824982357092449e-05,
      "loss": 4.6404,
      "step": 1000
    },
    {
      "epoch": 0.7127734650670431,
      "grad_norm": 9.441740989685059,
      "learning_rate": 3.813220418725006e-05,
      "loss": 4.4281,
      "step": 1010
    },
    {
      "epoch": 0.7198306280875089,
      "grad_norm": 16.977764129638672,
      "learning_rate": 3.8014584803575634e-05,
      "loss": 4.4505,
      "step": 1020
    },
    {
      "epoch": 0.7268877911079746,
      "grad_norm": 7.668999671936035,
      "learning_rate": 3.7896965419901206e-05,
      "loss": 4.6454,
      "step": 1030
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 8.546283721923828,
      "learning_rate": 3.777934603622677e-05,
      "loss": 4.678,
      "step": 1040
    },
    {
      "epoch": 0.7410021171489062,
      "grad_norm": 9.073867797851562,
      "learning_rate": 3.766172665255234e-05,
      "loss": 4.203,
      "step": 1050
    },
    {
      "epoch": 0.748059280169372,
      "grad_norm": 10.289641380310059,
      "learning_rate": 3.7544107268877915e-05,
      "loss": 4.1306,
      "step": 1060
    },
    {
      "epoch": 0.7551164431898377,
      "grad_norm": 11.478127479553223,
      "learning_rate": 3.742648788520348e-05,
      "loss": 4.4407,
      "step": 1070
    },
    {
      "epoch": 0.7621736062103035,
      "grad_norm": 9.32446575164795,
      "learning_rate": 3.730886850152905e-05,
      "loss": 4.2352,
      "step": 1080
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 8.859457969665527,
      "learning_rate": 3.7191249117854624e-05,
      "loss": 4.6367,
      "step": 1090
    },
    {
      "epoch": 0.776287932251235,
      "grad_norm": 8.488073348999023,
      "learning_rate": 3.7073629734180196e-05,
      "loss": 4.4008,
      "step": 1100
    },
    {
      "epoch": 0.7833450952717008,
      "grad_norm": 11.007185935974121,
      "learning_rate": 3.695601035050577e-05,
      "loss": 4.7841,
      "step": 1110
    },
    {
      "epoch": 0.7904022582921666,
      "grad_norm": 9.244623184204102,
      "learning_rate": 3.683839096683134e-05,
      "loss": 4.791,
      "step": 1120
    },
    {
      "epoch": 0.7974594213126324,
      "grad_norm": 9.541743278503418,
      "learning_rate": 3.6720771583156905e-05,
      "loss": 4.315,
      "step": 1130
    },
    {
      "epoch": 0.8045165843330981,
      "grad_norm": 8.969767570495605,
      "learning_rate": 3.660315219948248e-05,
      "loss": 4.3276,
      "step": 1140
    },
    {
      "epoch": 0.8115737473535639,
      "grad_norm": 9.796631813049316,
      "learning_rate": 3.648553281580805e-05,
      "loss": 4.7282,
      "step": 1150
    },
    {
      "epoch": 0.8186309103740297,
      "grad_norm": 8.522063255310059,
      "learning_rate": 3.6367913432133614e-05,
      "loss": 4.2806,
      "step": 1160
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 8.751221656799316,
      "learning_rate": 3.6250294048459186e-05,
      "loss": 4.2084,
      "step": 1170
    },
    {
      "epoch": 0.8327452364149612,
      "grad_norm": 20.337556838989258,
      "learning_rate": 3.613267466478476e-05,
      "loss": 4.1925,
      "step": 1180
    },
    {
      "epoch": 0.839802399435427,
      "grad_norm": 10.750216484069824,
      "learning_rate": 3.601505528111033e-05,
      "loss": 4.4,
      "step": 1190
    },
    {
      "epoch": 0.8468595624558928,
      "grad_norm": 9.204002380371094,
      "learning_rate": 3.58974358974359e-05,
      "loss": 4.6965,
      "step": 1200
    },
    {
      "epoch": 0.8539167254763586,
      "grad_norm": 9.752402305603027,
      "learning_rate": 3.5779816513761474e-05,
      "loss": 4.2628,
      "step": 1210
    },
    {
      "epoch": 0.8609738884968243,
      "grad_norm": 9.843485832214355,
      "learning_rate": 3.566219713008704e-05,
      "loss": 4.0312,
      "step": 1220
    },
    {
      "epoch": 0.8680310515172901,
      "grad_norm": 10.742966651916504,
      "learning_rate": 3.554457774641261e-05,
      "loss": 4.6003,
      "step": 1230
    },
    {
      "epoch": 0.8750882145377559,
      "grad_norm": 9.425711631774902,
      "learning_rate": 3.5426958362738183e-05,
      "loss": 4.6045,
      "step": 1240
    },
    {
      "epoch": 0.8821453775582216,
      "grad_norm": 9.675908088684082,
      "learning_rate": 3.530933897906375e-05,
      "loss": 4.2893,
      "step": 1250
    },
    {
      "epoch": 0.8892025405786874,
      "grad_norm": 9.927431106567383,
      "learning_rate": 3.519171959538932e-05,
      "loss": 4.4909,
      "step": 1260
    },
    {
      "epoch": 0.8962597035991532,
      "grad_norm": 8.468513488769531,
      "learning_rate": 3.507410021171489e-05,
      "loss": 4.4252,
      "step": 1270
    },
    {
      "epoch": 0.903316866619619,
      "grad_norm": 9.621182441711426,
      "learning_rate": 3.495648082804046e-05,
      "loss": 4.2707,
      "step": 1280
    },
    {
      "epoch": 0.9103740296400847,
      "grad_norm": 10.686822891235352,
      "learning_rate": 3.4838861444366037e-05,
      "loss": 4.4369,
      "step": 1290
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 10.1796293258667,
      "learning_rate": 3.472124206069161e-05,
      "loss": 4.6618,
      "step": 1300
    },
    {
      "epoch": 0.9244883556810163,
      "grad_norm": 9.444635391235352,
      "learning_rate": 3.4603622677017174e-05,
      "loss": 4.5198,
      "step": 1310
    },
    {
      "epoch": 0.9315455187014821,
      "grad_norm": 9.796131134033203,
      "learning_rate": 3.4486003293342746e-05,
      "loss": 4.4646,
      "step": 1320
    },
    {
      "epoch": 0.9386026817219478,
      "grad_norm": 10.485221862792969,
      "learning_rate": 3.436838390966832e-05,
      "loss": 4.365,
      "step": 1330
    },
    {
      "epoch": 0.9456598447424136,
      "grad_norm": 9.221111297607422,
      "learning_rate": 3.425076452599388e-05,
      "loss": 4.5638,
      "step": 1340
    },
    {
      "epoch": 0.9527170077628794,
      "grad_norm": 8.962937355041504,
      "learning_rate": 3.4133145142319455e-05,
      "loss": 4.048,
      "step": 1350
    },
    {
      "epoch": 0.9597741707833451,
      "grad_norm": 8.918802261352539,
      "learning_rate": 3.401552575864503e-05,
      "loss": 4.8599,
      "step": 1360
    },
    {
      "epoch": 0.9668313338038109,
      "grad_norm": 7.737545967102051,
      "learning_rate": 3.389790637497059e-05,
      "loss": 4.0644,
      "step": 1370
    },
    {
      "epoch": 0.9738884968242767,
      "grad_norm": 10.378456115722656,
      "learning_rate": 3.3780286991296164e-05,
      "loss": 4.5572,
      "step": 1380
    },
    {
      "epoch": 0.9809456598447425,
      "grad_norm": 9.053503036499023,
      "learning_rate": 3.366266760762174e-05,
      "loss": 4.055,
      "step": 1390
    },
    {
      "epoch": 0.9880028228652082,
      "grad_norm": 10.068299293518066,
      "learning_rate": 3.354504822394731e-05,
      "loss": 4.1076,
      "step": 1400
    },
    {
      "epoch": 0.995059985885674,
      "grad_norm": 10.465929985046387,
      "learning_rate": 3.342742884027288e-05,
      "loss": 4.6118,
      "step": 1410
    },
    {
      "epoch": 1.0021171489061398,
      "grad_norm": 8.2327299118042,
      "learning_rate": 3.330980945659845e-05,
      "loss": 3.7515,
      "step": 1420
    },
    {
      "epoch": 1.0091743119266054,
      "grad_norm": 8.136971473693848,
      "learning_rate": 3.319219007292402e-05,
      "loss": 3.0453,
      "step": 1430
    },
    {
      "epoch": 1.0162314749470713,
      "grad_norm": 9.341383934020996,
      "learning_rate": 3.307457068924959e-05,
      "loss": 3.0881,
      "step": 1440
    },
    {
      "epoch": 1.023288637967537,
      "grad_norm": 9.343704223632812,
      "learning_rate": 3.295695130557516e-05,
      "loss": 3.3557,
      "step": 1450
    },
    {
      "epoch": 1.0303458009880029,
      "grad_norm": 9.747456550598145,
      "learning_rate": 3.2839331921900726e-05,
      "loss": 3.8079,
      "step": 1460
    },
    {
      "epoch": 1.0374029640084685,
      "grad_norm": 9.489311218261719,
      "learning_rate": 3.27217125382263e-05,
      "loss": 3.0629,
      "step": 1470
    },
    {
      "epoch": 1.0444601270289344,
      "grad_norm": 8.800625801086426,
      "learning_rate": 3.260409315455187e-05,
      "loss": 3.5035,
      "step": 1480
    },
    {
      "epoch": 1.0515172900494,
      "grad_norm": 9.159561157226562,
      "learning_rate": 3.248647377087744e-05,
      "loss": 3.2981,
      "step": 1490
    },
    {
      "epoch": 1.058574453069866,
      "grad_norm": 9.634895324707031,
      "learning_rate": 3.2368854387203014e-05,
      "loss": 3.5994,
      "step": 1500
    },
    {
      "epoch": 1.0656316160903316,
      "grad_norm": 9.040266036987305,
      "learning_rate": 3.2251235003528586e-05,
      "loss": 3.2005,
      "step": 1510
    },
    {
      "epoch": 1.0726887791107975,
      "grad_norm": 9.062068939208984,
      "learning_rate": 3.213361561985415e-05,
      "loss": 3.4751,
      "step": 1520
    },
    {
      "epoch": 1.0797459421312632,
      "grad_norm": 9.769621849060059,
      "learning_rate": 3.2015996236179724e-05,
      "loss": 3.5793,
      "step": 1530
    },
    {
      "epoch": 1.086803105151729,
      "grad_norm": 9.52597427368164,
      "learning_rate": 3.1898376852505296e-05,
      "loss": 3.6133,
      "step": 1540
    },
    {
      "epoch": 1.0938602681721947,
      "grad_norm": 10.37137222290039,
      "learning_rate": 3.178075746883086e-05,
      "loss": 3.1594,
      "step": 1550
    },
    {
      "epoch": 1.1009174311926606,
      "grad_norm": 9.489562034606934,
      "learning_rate": 3.166313808515643e-05,
      "loss": 3.3398,
      "step": 1560
    },
    {
      "epoch": 1.1079745942131263,
      "grad_norm": 9.20508098602295,
      "learning_rate": 3.1545518701482005e-05,
      "loss": 3.2232,
      "step": 1570
    },
    {
      "epoch": 1.1150317572335922,
      "grad_norm": 8.731060981750488,
      "learning_rate": 3.142789931780758e-05,
      "loss": 3.4444,
      "step": 1580
    },
    {
      "epoch": 1.1220889202540578,
      "grad_norm": 9.069389343261719,
      "learning_rate": 3.131027993413315e-05,
      "loss": 3.1688,
      "step": 1590
    },
    {
      "epoch": 1.1291460832745237,
      "grad_norm": 10.623867988586426,
      "learning_rate": 3.119266055045872e-05,
      "loss": 3.2492,
      "step": 1600
    },
    {
      "epoch": 1.1362032462949894,
      "grad_norm": 9.495043754577637,
      "learning_rate": 3.1075041166784286e-05,
      "loss": 3.4973,
      "step": 1610
    },
    {
      "epoch": 1.1432604093154553,
      "grad_norm": 9.021018028259277,
      "learning_rate": 3.095742178310986e-05,
      "loss": 3.3178,
      "step": 1620
    },
    {
      "epoch": 1.150317572335921,
      "grad_norm": 7.946266174316406,
      "learning_rate": 3.083980239943543e-05,
      "loss": 3.18,
      "step": 1630
    },
    {
      "epoch": 1.1573747353563868,
      "grad_norm": 10.671736717224121,
      "learning_rate": 3.0722183015760995e-05,
      "loss": 3.265,
      "step": 1640
    },
    {
      "epoch": 1.1644318983768525,
      "grad_norm": 12.78036880493164,
      "learning_rate": 3.060456363208657e-05,
      "loss": 3.3846,
      "step": 1650
    },
    {
      "epoch": 1.1714890613973183,
      "grad_norm": 8.784818649291992,
      "learning_rate": 3.0486944248412142e-05,
      "loss": 3.1213,
      "step": 1660
    },
    {
      "epoch": 1.178546224417784,
      "grad_norm": 8.47570514678955,
      "learning_rate": 3.0369324864737708e-05,
      "loss": 3.2126,
      "step": 1670
    },
    {
      "epoch": 1.18560338743825,
      "grad_norm": 11.963545799255371,
      "learning_rate": 3.025170548106328e-05,
      "loss": 3.5155,
      "step": 1680
    },
    {
      "epoch": 1.1926605504587156,
      "grad_norm": 8.794167518615723,
      "learning_rate": 3.013408609738885e-05,
      "loss": 3.3053,
      "step": 1690
    },
    {
      "epoch": 1.1997177134791814,
      "grad_norm": 11.49416446685791,
      "learning_rate": 3.001646671371442e-05,
      "loss": 3.0822,
      "step": 1700
    },
    {
      "epoch": 1.206774876499647,
      "grad_norm": 9.592041015625,
      "learning_rate": 2.9898847330039992e-05,
      "loss": 3.3835,
      "step": 1710
    },
    {
      "epoch": 1.213832039520113,
      "grad_norm": 7.595232009887695,
      "learning_rate": 2.9781227946365564e-05,
      "loss": 2.6767,
      "step": 1720
    },
    {
      "epoch": 1.2208892025405786,
      "grad_norm": 9.92708683013916,
      "learning_rate": 2.966360856269113e-05,
      "loss": 3.5048,
      "step": 1730
    },
    {
      "epoch": 1.2279463655610445,
      "grad_norm": 7.507707595825195,
      "learning_rate": 2.9545989179016705e-05,
      "loss": 3.1566,
      "step": 1740
    },
    {
      "epoch": 1.2350035285815102,
      "grad_norm": 9.44698429107666,
      "learning_rate": 2.9428369795342277e-05,
      "loss": 3.563,
      "step": 1750
    },
    {
      "epoch": 1.242060691601976,
      "grad_norm": 9.25339412689209,
      "learning_rate": 2.9310750411667842e-05,
      "loss": 3.1339,
      "step": 1760
    },
    {
      "epoch": 1.2491178546224417,
      "grad_norm": 9.110088348388672,
      "learning_rate": 2.9193131027993414e-05,
      "loss": 3.2524,
      "step": 1770
    },
    {
      "epoch": 1.2561750176429076,
      "grad_norm": 10.379889488220215,
      "learning_rate": 2.9075511644318986e-05,
      "loss": 3.3305,
      "step": 1780
    },
    {
      "epoch": 1.2632321806633733,
      "grad_norm": 10.049041748046875,
      "learning_rate": 2.8957892260644554e-05,
      "loss": 3.27,
      "step": 1790
    },
    {
      "epoch": 1.2702893436838392,
      "grad_norm": 8.982878684997559,
      "learning_rate": 2.8840272876970126e-05,
      "loss": 2.9677,
      "step": 1800
    },
    {
      "epoch": 1.2773465067043048,
      "grad_norm": 9.689566612243652,
      "learning_rate": 2.87226534932957e-05,
      "loss": 3.4378,
      "step": 1810
    },
    {
      "epoch": 1.2844036697247707,
      "grad_norm": 11.137922286987305,
      "learning_rate": 2.8605034109621264e-05,
      "loss": 2.8093,
      "step": 1820
    },
    {
      "epoch": 1.2914608327452364,
      "grad_norm": 11.461560249328613,
      "learning_rate": 2.8487414725946836e-05,
      "loss": 3.3381,
      "step": 1830
    },
    {
      "epoch": 1.2985179957657023,
      "grad_norm": 9.8004732131958,
      "learning_rate": 2.836979534227241e-05,
      "loss": 3.3936,
      "step": 1840
    },
    {
      "epoch": 1.305575158786168,
      "grad_norm": 8.054906845092773,
      "learning_rate": 2.8252175958597976e-05,
      "loss": 3.3165,
      "step": 1850
    },
    {
      "epoch": 1.3126323218066338,
      "grad_norm": 9.269681930541992,
      "learning_rate": 2.8134556574923548e-05,
      "loss": 2.9753,
      "step": 1860
    },
    {
      "epoch": 1.3196894848270995,
      "grad_norm": 9.171138763427734,
      "learning_rate": 2.801693719124912e-05,
      "loss": 3.2608,
      "step": 1870
    },
    {
      "epoch": 1.3267466478475654,
      "grad_norm": 10.413583755493164,
      "learning_rate": 2.789931780757469e-05,
      "loss": 3.1647,
      "step": 1880
    },
    {
      "epoch": 1.333803810868031,
      "grad_norm": 8.572402954101562,
      "learning_rate": 2.778169842390026e-05,
      "loss": 3.1892,
      "step": 1890
    },
    {
      "epoch": 1.340860973888497,
      "grad_norm": 7.819873809814453,
      "learning_rate": 2.7664079040225833e-05,
      "loss": 3.1809,
      "step": 1900
    },
    {
      "epoch": 1.3479181369089626,
      "grad_norm": 12.187178611755371,
      "learning_rate": 2.7546459656551398e-05,
      "loss": 3.6215,
      "step": 1910
    },
    {
      "epoch": 1.3549752999294284,
      "grad_norm": 9.430079460144043,
      "learning_rate": 2.742884027287697e-05,
      "loss": 3.1414,
      "step": 1920
    },
    {
      "epoch": 1.362032462949894,
      "grad_norm": 7.060177803039551,
      "learning_rate": 2.7311220889202542e-05,
      "loss": 2.9197,
      "step": 1930
    },
    {
      "epoch": 1.36908962597036,
      "grad_norm": 7.912906646728516,
      "learning_rate": 2.719360150552811e-05,
      "loss": 3.3192,
      "step": 1940
    },
    {
      "epoch": 1.3761467889908257,
      "grad_norm": 11.055102348327637,
      "learning_rate": 2.7075982121853682e-05,
      "loss": 3.3206,
      "step": 1950
    },
    {
      "epoch": 1.3832039520112915,
      "grad_norm": 10.177406311035156,
      "learning_rate": 2.6958362738179254e-05,
      "loss": 3.3881,
      "step": 1960
    },
    {
      "epoch": 1.3902611150317572,
      "grad_norm": 8.931294441223145,
      "learning_rate": 2.6840743354504823e-05,
      "loss": 3.1955,
      "step": 1970
    },
    {
      "epoch": 1.397318278052223,
      "grad_norm": 9.865416526794434,
      "learning_rate": 2.6723123970830395e-05,
      "loss": 3.37,
      "step": 1980
    },
    {
      "epoch": 1.4043754410726887,
      "grad_norm": 7.6132659912109375,
      "learning_rate": 2.6605504587155967e-05,
      "loss": 3.2912,
      "step": 1990
    },
    {
      "epoch": 1.4114326040931546,
      "grad_norm": 10.367283821105957,
      "learning_rate": 2.6487885203481532e-05,
      "loss": 3.445,
      "step": 2000
    },
    {
      "epoch": 1.4184897671136203,
      "grad_norm": 10.666753768920898,
      "learning_rate": 2.6370265819807104e-05,
      "loss": 3.4661,
      "step": 2010
    },
    {
      "epoch": 1.4255469301340862,
      "grad_norm": 8.128179550170898,
      "learning_rate": 2.6252646436132676e-05,
      "loss": 2.7361,
      "step": 2020
    },
    {
      "epoch": 1.4326040931545518,
      "grad_norm": 7.9328179359436035,
      "learning_rate": 2.6135027052458245e-05,
      "loss": 3.2388,
      "step": 2030
    },
    {
      "epoch": 1.4396612561750177,
      "grad_norm": 8.83342170715332,
      "learning_rate": 2.6017407668783817e-05,
      "loss": 3.0454,
      "step": 2040
    },
    {
      "epoch": 1.4467184191954834,
      "grad_norm": 9.105942726135254,
      "learning_rate": 2.589978828510939e-05,
      "loss": 3.1927,
      "step": 2050
    },
    {
      "epoch": 1.4537755822159493,
      "grad_norm": 11.104310989379883,
      "learning_rate": 2.5782168901434954e-05,
      "loss": 3.0924,
      "step": 2060
    },
    {
      "epoch": 1.460832745236415,
      "grad_norm": 9.372265815734863,
      "learning_rate": 2.566454951776053e-05,
      "loss": 3.1173,
      "step": 2070
    },
    {
      "epoch": 1.4678899082568808,
      "grad_norm": 9.45535945892334,
      "learning_rate": 2.55469301340861e-05,
      "loss": 3.6022,
      "step": 2080
    },
    {
      "epoch": 1.4749470712773465,
      "grad_norm": 10.982789039611816,
      "learning_rate": 2.5429310750411667e-05,
      "loss": 3.1189,
      "step": 2090
    },
    {
      "epoch": 1.4820042342978124,
      "grad_norm": 11.055499076843262,
      "learning_rate": 2.531169136673724e-05,
      "loss": 3.2016,
      "step": 2100
    },
    {
      "epoch": 1.489061397318278,
      "grad_norm": 8.667793273925781,
      "learning_rate": 2.519407198306281e-05,
      "loss": 3.0477,
      "step": 2110
    },
    {
      "epoch": 1.496118560338744,
      "grad_norm": 10.937450408935547,
      "learning_rate": 2.507645259938838e-05,
      "loss": 3.5558,
      "step": 2120
    },
    {
      "epoch": 1.5031757233592096,
      "grad_norm": 12.143418312072754,
      "learning_rate": 2.495883321571395e-05,
      "loss": 3.4247,
      "step": 2130
    },
    {
      "epoch": 1.5102328863796752,
      "grad_norm": 9.441715240478516,
      "learning_rate": 2.4841213832039523e-05,
      "loss": 3.054,
      "step": 2140
    },
    {
      "epoch": 1.5172900494001411,
      "grad_norm": 10.67164134979248,
      "learning_rate": 2.472359444836509e-05,
      "loss": 3.2803,
      "step": 2150
    },
    {
      "epoch": 1.524347212420607,
      "grad_norm": 9.373043060302734,
      "learning_rate": 2.460597506469066e-05,
      "loss": 3.0506,
      "step": 2160
    },
    {
      "epoch": 1.5314043754410727,
      "grad_norm": 10.193906784057617,
      "learning_rate": 2.4488355681016236e-05,
      "loss": 3.2196,
      "step": 2170
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 11.146809577941895,
      "learning_rate": 2.4370736297341804e-05,
      "loss": 2.9717,
      "step": 2180
    },
    {
      "epoch": 1.5455187014820042,
      "grad_norm": 8.299763679504395,
      "learning_rate": 2.4253116913667373e-05,
      "loss": 2.9496,
      "step": 2190
    },
    {
      "epoch": 1.55257586450247,
      "grad_norm": 10.81899356842041,
      "learning_rate": 2.4135497529992945e-05,
      "loss": 3.0303,
      "step": 2200
    },
    {
      "epoch": 1.5596330275229358,
      "grad_norm": 8.924066543579102,
      "learning_rate": 2.4017878146318513e-05,
      "loss": 3.1408,
      "step": 2210
    },
    {
      "epoch": 1.5666901905434014,
      "grad_norm": 9.722600936889648,
      "learning_rate": 2.3900258762644085e-05,
      "loss": 3.6472,
      "step": 2220
    },
    {
      "epoch": 1.5737473535638673,
      "grad_norm": 8.678665161132812,
      "learning_rate": 2.3782639378969657e-05,
      "loss": 2.9751,
      "step": 2230
    },
    {
      "epoch": 1.5808045165843332,
      "grad_norm": 10.651862144470215,
      "learning_rate": 2.3665019995295226e-05,
      "loss": 3.1346,
      "step": 2240
    },
    {
      "epoch": 1.5878616796047988,
      "grad_norm": 12.816217422485352,
      "learning_rate": 2.3547400611620795e-05,
      "loss": 3.4783,
      "step": 2250
    },
    {
      "epoch": 1.5949188426252645,
      "grad_norm": 9.408286094665527,
      "learning_rate": 2.3429781227946367e-05,
      "loss": 3.6993,
      "step": 2260
    },
    {
      "epoch": 1.6019760056457304,
      "grad_norm": 9.457021713256836,
      "learning_rate": 2.331216184427194e-05,
      "loss": 3.3538,
      "step": 2270
    },
    {
      "epoch": 1.6090331686661963,
      "grad_norm": 9.672845840454102,
      "learning_rate": 2.3194542460597507e-05,
      "loss": 3.0079,
      "step": 2280
    },
    {
      "epoch": 1.616090331686662,
      "grad_norm": 8.692377090454102,
      "learning_rate": 2.307692307692308e-05,
      "loss": 3.5226,
      "step": 2290
    },
    {
      "epoch": 1.6231474947071276,
      "grad_norm": 10.768632888793945,
      "learning_rate": 2.2959303693248648e-05,
      "loss": 3.4396,
      "step": 2300
    },
    {
      "epoch": 1.6302046577275935,
      "grad_norm": 11.301620483398438,
      "learning_rate": 2.284168430957422e-05,
      "loss": 3.5437,
      "step": 2310
    },
    {
      "epoch": 1.6372618207480594,
      "grad_norm": 9.935906410217285,
      "learning_rate": 2.272406492589979e-05,
      "loss": 2.8911,
      "step": 2320
    },
    {
      "epoch": 1.644318983768525,
      "grad_norm": 8.616249084472656,
      "learning_rate": 2.260644554222536e-05,
      "loss": 2.6739,
      "step": 2330
    },
    {
      "epoch": 1.6513761467889907,
      "grad_norm": 9.840902328491211,
      "learning_rate": 2.248882615855093e-05,
      "loss": 3.3054,
      "step": 2340
    },
    {
      "epoch": 1.6584333098094566,
      "grad_norm": 10.59205150604248,
      "learning_rate": 2.23712067748765e-05,
      "loss": 3.3512,
      "step": 2350
    },
    {
      "epoch": 1.6654904728299225,
      "grad_norm": 8.032137870788574,
      "learning_rate": 2.2253587391202073e-05,
      "loss": 3.1585,
      "step": 2360
    },
    {
      "epoch": 1.6725476358503881,
      "grad_norm": 9.984152793884277,
      "learning_rate": 2.213596800752764e-05,
      "loss": 3.3384,
      "step": 2370
    },
    {
      "epoch": 1.6796047988708538,
      "grad_norm": 9.333093643188477,
      "learning_rate": 2.2018348623853213e-05,
      "loss": 3.1832,
      "step": 2380
    },
    {
      "epoch": 1.6866619618913197,
      "grad_norm": 8.905866622924805,
      "learning_rate": 2.1900729240178782e-05,
      "loss": 3.0662,
      "step": 2390
    },
    {
      "epoch": 1.6937191249117856,
      "grad_norm": 10.644828796386719,
      "learning_rate": 2.1783109856504354e-05,
      "loss": 3.1313,
      "step": 2400
    },
    {
      "epoch": 1.7007762879322512,
      "grad_norm": 10.560699462890625,
      "learning_rate": 2.1665490472829926e-05,
      "loss": 2.8501,
      "step": 2410
    },
    {
      "epoch": 1.7078334509527169,
      "grad_norm": 11.571292877197266,
      "learning_rate": 2.1547871089155495e-05,
      "loss": 3.6606,
      "step": 2420
    },
    {
      "epoch": 1.7148906139731828,
      "grad_norm": 10.100242614746094,
      "learning_rate": 2.1430251705481063e-05,
      "loss": 2.8483,
      "step": 2430
    },
    {
      "epoch": 1.7219477769936486,
      "grad_norm": 7.932751655578613,
      "learning_rate": 2.1312632321806635e-05,
      "loss": 2.5609,
      "step": 2440
    },
    {
      "epoch": 1.7290049400141143,
      "grad_norm": 9.209373474121094,
      "learning_rate": 2.1195012938132207e-05,
      "loss": 2.4713,
      "step": 2450
    },
    {
      "epoch": 1.73606210303458,
      "grad_norm": 10.494736671447754,
      "learning_rate": 2.1077393554457776e-05,
      "loss": 2.54,
      "step": 2460
    },
    {
      "epoch": 1.7431192660550459,
      "grad_norm": 11.318928718566895,
      "learning_rate": 2.0959774170783348e-05,
      "loss": 3.5764,
      "step": 2470
    },
    {
      "epoch": 1.7501764290755117,
      "grad_norm": 8.998852729797363,
      "learning_rate": 2.0842154787108916e-05,
      "loss": 3.5373,
      "step": 2480
    },
    {
      "epoch": 1.7572335920959774,
      "grad_norm": 9.168340682983398,
      "learning_rate": 2.0724535403434485e-05,
      "loss": 3.3622,
      "step": 2490
    },
    {
      "epoch": 1.764290755116443,
      "grad_norm": 7.895203590393066,
      "learning_rate": 2.060691601976006e-05,
      "loss": 2.7737,
      "step": 2500
    },
    {
      "epoch": 1.771347918136909,
      "grad_norm": 8.677999496459961,
      "learning_rate": 2.048929663608563e-05,
      "loss": 3.2876,
      "step": 2510
    },
    {
      "epoch": 1.7784050811573748,
      "grad_norm": 8.814691543579102,
      "learning_rate": 2.0371677252411197e-05,
      "loss": 2.7338,
      "step": 2520
    },
    {
      "epoch": 1.7854622441778405,
      "grad_norm": 8.987536430358887,
      "learning_rate": 2.025405786873677e-05,
      "loss": 3.3134,
      "step": 2530
    },
    {
      "epoch": 1.7925194071983062,
      "grad_norm": 9.056525230407715,
      "learning_rate": 2.0136438485062338e-05,
      "loss": 3.2215,
      "step": 2540
    },
    {
      "epoch": 1.799576570218772,
      "grad_norm": 7.500690460205078,
      "learning_rate": 2.001881910138791e-05,
      "loss": 3.0919,
      "step": 2550
    },
    {
      "epoch": 1.806633733239238,
      "grad_norm": 11.403131484985352,
      "learning_rate": 1.9901199717713482e-05,
      "loss": 3.3601,
      "step": 2560
    },
    {
      "epoch": 1.8136908962597036,
      "grad_norm": 10.890182495117188,
      "learning_rate": 1.978358033403905e-05,
      "loss": 3.5398,
      "step": 2570
    },
    {
      "epoch": 1.8207480592801693,
      "grad_norm": 7.920186996459961,
      "learning_rate": 1.966596095036462e-05,
      "loss": 3.1623,
      "step": 2580
    },
    {
      "epoch": 1.8278052223006351,
      "grad_norm": 8.668757438659668,
      "learning_rate": 1.954834156669019e-05,
      "loss": 3.0334,
      "step": 2590
    },
    {
      "epoch": 1.834862385321101,
      "grad_norm": 7.825656414031982,
      "learning_rate": 1.9430722183015763e-05,
      "loss": 2.7953,
      "step": 2600
    },
    {
      "epoch": 1.8419195483415667,
      "grad_norm": 10.514918327331543,
      "learning_rate": 1.9313102799341332e-05,
      "loss": 2.9482,
      "step": 2610
    },
    {
      "epoch": 1.8489767113620323,
      "grad_norm": 10.487438201904297,
      "learning_rate": 1.9195483415666904e-05,
      "loss": 3.518,
      "step": 2620
    },
    {
      "epoch": 1.8560338743824982,
      "grad_norm": 8.738203048706055,
      "learning_rate": 1.9077864031992472e-05,
      "loss": 3.2263,
      "step": 2630
    },
    {
      "epoch": 1.8630910374029641,
      "grad_norm": 9.8330078125,
      "learning_rate": 1.8960244648318044e-05,
      "loss": 3.3114,
      "step": 2640
    },
    {
      "epoch": 1.8701482004234298,
      "grad_norm": 10.00748348236084,
      "learning_rate": 1.8842625264643616e-05,
      "loss": 3.2407,
      "step": 2650
    },
    {
      "epoch": 1.8772053634438954,
      "grad_norm": 9.536447525024414,
      "learning_rate": 1.8725005880969185e-05,
      "loss": 2.9083,
      "step": 2660
    },
    {
      "epoch": 1.8842625264643613,
      "grad_norm": 11.740077018737793,
      "learning_rate": 1.8607386497294753e-05,
      "loss": 3.5827,
      "step": 2670
    },
    {
      "epoch": 1.8913196894848272,
      "grad_norm": 9.999406814575195,
      "learning_rate": 1.8489767113620325e-05,
      "loss": 3.3498,
      "step": 2680
    },
    {
      "epoch": 1.8983768525052929,
      "grad_norm": 9.886404037475586,
      "learning_rate": 1.8372147729945897e-05,
      "loss": 3.3939,
      "step": 2690
    },
    {
      "epoch": 1.9054340155257585,
      "grad_norm": 10.254223823547363,
      "learning_rate": 1.8254528346271466e-05,
      "loss": 3.2577,
      "step": 2700
    },
    {
      "epoch": 1.9124911785462244,
      "grad_norm": 10.898235321044922,
      "learning_rate": 1.8136908962597038e-05,
      "loss": 3.2139,
      "step": 2710
    },
    {
      "epoch": 1.9195483415666903,
      "grad_norm": 9.844694137573242,
      "learning_rate": 1.8019289578922607e-05,
      "loss": 3.2766,
      "step": 2720
    },
    {
      "epoch": 1.926605504587156,
      "grad_norm": 11.458808898925781,
      "learning_rate": 1.790167019524818e-05,
      "loss": 3.1878,
      "step": 2730
    },
    {
      "epoch": 1.9336626676076216,
      "grad_norm": 8.555170059204102,
      "learning_rate": 1.778405081157375e-05,
      "loss": 3.4203,
      "step": 2740
    },
    {
      "epoch": 1.9407198306280875,
      "grad_norm": 8.478745460510254,
      "learning_rate": 1.766643142789932e-05,
      "loss": 3.1935,
      "step": 2750
    },
    {
      "epoch": 1.9477769936485534,
      "grad_norm": 8.36948013305664,
      "learning_rate": 1.7548812044224888e-05,
      "loss": 3.1638,
      "step": 2760
    },
    {
      "epoch": 1.954834156669019,
      "grad_norm": 9.765765190124512,
      "learning_rate": 1.743119266055046e-05,
      "loss": 3.3895,
      "step": 2770
    },
    {
      "epoch": 1.9618913196894847,
      "grad_norm": 8.208702087402344,
      "learning_rate": 1.7313573276876032e-05,
      "loss": 3.1535,
      "step": 2780
    },
    {
      "epoch": 1.9689484827099506,
      "grad_norm": 10.946487426757812,
      "learning_rate": 1.71959538932016e-05,
      "loss": 3.8771,
      "step": 2790
    },
    {
      "epoch": 1.9760056457304165,
      "grad_norm": 11.11671257019043,
      "learning_rate": 1.7078334509527172e-05,
      "loss": 2.9497,
      "step": 2800
    },
    {
      "epoch": 1.9830628087508821,
      "grad_norm": 7.591213703155518,
      "learning_rate": 1.696071512585274e-05,
      "loss": 2.7794,
      "step": 2810
    },
    {
      "epoch": 1.9901199717713478,
      "grad_norm": 7.497946739196777,
      "learning_rate": 1.684309574217831e-05,
      "loss": 2.7743,
      "step": 2820
    },
    {
      "epoch": 1.9971771347918137,
      "grad_norm": 9.849123001098633,
      "learning_rate": 1.6725476358503885e-05,
      "loss": 3.5489,
      "step": 2830
    }
  ],
  "logging_steps": 10,
  "max_steps": 4251,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 185125404672000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
