{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1417,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007057163020465773,
      "grad_norm": 9.803939819335938,
      "learning_rate": 4.989414255469302e-05,
      "loss": 5.7008,
      "step": 10
    },
    {
      "epoch": 0.014114326040931546,
      "grad_norm": 14.286739349365234,
      "learning_rate": 4.9776523171018584e-05,
      "loss": 5.59,
      "step": 20
    },
    {
      "epoch": 0.02117148906139732,
      "grad_norm": 9.54956340789795,
      "learning_rate": 4.9658903787344156e-05,
      "loss": 5.2107,
      "step": 30
    },
    {
      "epoch": 0.028228652081863093,
      "grad_norm": 9.296425819396973,
      "learning_rate": 4.954128440366973e-05,
      "loss": 5.1773,
      "step": 40
    },
    {
      "epoch": 0.035285815102328866,
      "grad_norm": 9.932609558105469,
      "learning_rate": 4.9423665019995294e-05,
      "loss": 5.1755,
      "step": 50
    },
    {
      "epoch": 0.04234297812279464,
      "grad_norm": 9.96810245513916,
      "learning_rate": 4.9306045636320866e-05,
      "loss": 5.0035,
      "step": 60
    },
    {
      "epoch": 0.04940014114326041,
      "grad_norm": 9.972799301147461,
      "learning_rate": 4.9188426252646444e-05,
      "loss": 5.345,
      "step": 70
    },
    {
      "epoch": 0.056457304163726185,
      "grad_norm": 12.471413612365723,
      "learning_rate": 4.907080686897201e-05,
      "loss": 5.0411,
      "step": 80
    },
    {
      "epoch": 0.06351446718419196,
      "grad_norm": 9.28343677520752,
      "learning_rate": 4.895318748529758e-05,
      "loss": 5.1438,
      "step": 90
    },
    {
      "epoch": 0.07057163020465773,
      "grad_norm": 9.345253944396973,
      "learning_rate": 4.8835568101623154e-05,
      "loss": 4.9463,
      "step": 100
    },
    {
      "epoch": 0.0776287932251235,
      "grad_norm": 9.808815956115723,
      "learning_rate": 4.871794871794872e-05,
      "loss": 5.1154,
      "step": 110
    },
    {
      "epoch": 0.08468595624558928,
      "grad_norm": 9.499787330627441,
      "learning_rate": 4.860032933427429e-05,
      "loss": 4.9691,
      "step": 120
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 10.887115478515625,
      "learning_rate": 4.848270995059986e-05,
      "loss": 5.1977,
      "step": 130
    },
    {
      "epoch": 0.09880028228652082,
      "grad_norm": 9.585956573486328,
      "learning_rate": 4.836509056692543e-05,
      "loss": 4.8795,
      "step": 140
    },
    {
      "epoch": 0.1058574453069866,
      "grad_norm": 10.024253845214844,
      "learning_rate": 4.8247471183251e-05,
      "loss": 5.0115,
      "step": 150
    },
    {
      "epoch": 0.11291460832745237,
      "grad_norm": 10.05254077911377,
      "learning_rate": 4.812985179957657e-05,
      "loss": 5.0347,
      "step": 160
    },
    {
      "epoch": 0.11997177134791814,
      "grad_norm": 9.168549537658691,
      "learning_rate": 4.8012232415902144e-05,
      "loss": 5.2137,
      "step": 170
    },
    {
      "epoch": 0.12702893436838392,
      "grad_norm": 10.453447341918945,
      "learning_rate": 4.7894613032227716e-05,
      "loss": 5.047,
      "step": 180
    },
    {
      "epoch": 0.1340860973888497,
      "grad_norm": 8.862540245056152,
      "learning_rate": 4.777699364855329e-05,
      "loss": 5.0782,
      "step": 190
    },
    {
      "epoch": 0.14114326040931546,
      "grad_norm": 8.496728897094727,
      "learning_rate": 4.765937426487885e-05,
      "loss": 4.7925,
      "step": 200
    },
    {
      "epoch": 0.14820042342978124,
      "grad_norm": 9.522027969360352,
      "learning_rate": 4.7541754881204425e-05,
      "loss": 4.5334,
      "step": 210
    },
    {
      "epoch": 0.155257586450247,
      "grad_norm": 10.281990051269531,
      "learning_rate": 4.742413549753e-05,
      "loss": 4.8138,
      "step": 220
    },
    {
      "epoch": 0.16231474947071278,
      "grad_norm": 10.3073148727417,
      "learning_rate": 4.730651611385556e-05,
      "loss": 4.8046,
      "step": 230
    },
    {
      "epoch": 0.16937191249117856,
      "grad_norm": 9.511117935180664,
      "learning_rate": 4.7188896730181134e-05,
      "loss": 5.0472,
      "step": 240
    },
    {
      "epoch": 0.17642907551164433,
      "grad_norm": 9.618687629699707,
      "learning_rate": 4.7071277346506706e-05,
      "loss": 4.8456,
      "step": 250
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 8.858317375183105,
      "learning_rate": 4.695365796283228e-05,
      "loss": 4.8941,
      "step": 260
    },
    {
      "epoch": 0.19054340155257588,
      "grad_norm": 10.215332984924316,
      "learning_rate": 4.683603857915785e-05,
      "loss": 4.801,
      "step": 270
    },
    {
      "epoch": 0.19760056457304165,
      "grad_norm": 9.842853546142578,
      "learning_rate": 4.671841919548342e-05,
      "loss": 4.8424,
      "step": 280
    },
    {
      "epoch": 0.20465772759350742,
      "grad_norm": 7.9486918449401855,
      "learning_rate": 4.660079981180899e-05,
      "loss": 4.6034,
      "step": 290
    },
    {
      "epoch": 0.2117148906139732,
      "grad_norm": 10.1187744140625,
      "learning_rate": 4.648318042813456e-05,
      "loss": 4.6096,
      "step": 300
    },
    {
      "epoch": 0.21877205363443897,
      "grad_norm": 9.774480819702148,
      "learning_rate": 4.636556104446013e-05,
      "loss": 4.8018,
      "step": 310
    },
    {
      "epoch": 0.22582921665490474,
      "grad_norm": 9.91816520690918,
      "learning_rate": 4.6247941660785696e-05,
      "loss": 4.7862,
      "step": 320
    },
    {
      "epoch": 0.23288637967537051,
      "grad_norm": 9.578490257263184,
      "learning_rate": 4.613032227711127e-05,
      "loss": 4.9187,
      "step": 330
    },
    {
      "epoch": 0.2399435426958363,
      "grad_norm": 9.230071067810059,
      "learning_rate": 4.601270289343684e-05,
      "loss": 4.6422,
      "step": 340
    },
    {
      "epoch": 0.24700070571630206,
      "grad_norm": 10.706733703613281,
      "learning_rate": 4.589508350976241e-05,
      "loss": 4.7011,
      "step": 350
    },
    {
      "epoch": 0.25405786873676783,
      "grad_norm": 8.942960739135742,
      "learning_rate": 4.5777464126087984e-05,
      "loss": 4.9398,
      "step": 360
    },
    {
      "epoch": 0.2611150317572336,
      "grad_norm": 9.923922538757324,
      "learning_rate": 4.5659844742413556e-05,
      "loss": 5.1796,
      "step": 370
    },
    {
      "epoch": 0.2681721947776994,
      "grad_norm": 9.513066291809082,
      "learning_rate": 4.554222535873912e-05,
      "loss": 5.056,
      "step": 380
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 9.730828285217285,
      "learning_rate": 4.5424605975064694e-05,
      "loss": 5.0605,
      "step": 390
    },
    {
      "epoch": 0.2822865208186309,
      "grad_norm": 9.58809757232666,
      "learning_rate": 4.5306986591390266e-05,
      "loss": 5.0821,
      "step": 400
    },
    {
      "epoch": 0.2893436838390967,
      "grad_norm": 9.873540878295898,
      "learning_rate": 4.518936720771583e-05,
      "loss": 5.0323,
      "step": 410
    },
    {
      "epoch": 0.2964008468595625,
      "grad_norm": 9.330329895019531,
      "learning_rate": 4.50717478240414e-05,
      "loss": 4.5335,
      "step": 420
    },
    {
      "epoch": 0.30345800988002825,
      "grad_norm": 10.289673805236816,
      "learning_rate": 4.4954128440366975e-05,
      "loss": 4.9622,
      "step": 430
    },
    {
      "epoch": 0.310515172900494,
      "grad_norm": 9.374435424804688,
      "learning_rate": 4.483650905669255e-05,
      "loss": 4.788,
      "step": 440
    },
    {
      "epoch": 0.3175723359209598,
      "grad_norm": 9.762895584106445,
      "learning_rate": 4.471888967301812e-05,
      "loss": 4.9906,
      "step": 450
    },
    {
      "epoch": 0.32462949894142556,
      "grad_norm": 11.105172157287598,
      "learning_rate": 4.460127028934369e-05,
      "loss": 4.7867,
      "step": 460
    },
    {
      "epoch": 0.33168666196189134,
      "grad_norm": 9.769625663757324,
      "learning_rate": 4.4483650905669256e-05,
      "loss": 4.6602,
      "step": 470
    },
    {
      "epoch": 0.3387438249823571,
      "grad_norm": 9.89722728729248,
      "learning_rate": 4.436603152199483e-05,
      "loss": 4.5194,
      "step": 480
    },
    {
      "epoch": 0.3458009880028229,
      "grad_norm": 8.95580768585205,
      "learning_rate": 4.42484121383204e-05,
      "loss": 4.5984,
      "step": 490
    },
    {
      "epoch": 0.35285815102328866,
      "grad_norm": 9.174824714660645,
      "learning_rate": 4.4130792754645965e-05,
      "loss": 4.7109,
      "step": 500
    },
    {
      "epoch": 0.35991531404375443,
      "grad_norm": 10.257124900817871,
      "learning_rate": 4.401317337097154e-05,
      "loss": 4.7568,
      "step": 510
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 9.179488182067871,
      "learning_rate": 4.389555398729711e-05,
      "loss": 4.6118,
      "step": 520
    },
    {
      "epoch": 0.374029640084686,
      "grad_norm": 9.112079620361328,
      "learning_rate": 4.377793460362268e-05,
      "loss": 5.1196,
      "step": 530
    },
    {
      "epoch": 0.38108680310515175,
      "grad_norm": 9.738810539245605,
      "learning_rate": 4.366031521994825e-05,
      "loss": 4.9859,
      "step": 540
    },
    {
      "epoch": 0.3881439661256175,
      "grad_norm": 10.382537841796875,
      "learning_rate": 4.3542695836273825e-05,
      "loss": 4.7305,
      "step": 550
    },
    {
      "epoch": 0.3952011291460833,
      "grad_norm": 9.557065963745117,
      "learning_rate": 4.342507645259939e-05,
      "loss": 4.7168,
      "step": 560
    },
    {
      "epoch": 0.40225829216654907,
      "grad_norm": 10.748839378356934,
      "learning_rate": 4.330745706892496e-05,
      "loss": 4.5533,
      "step": 570
    },
    {
      "epoch": 0.40931545518701484,
      "grad_norm": 10.571599006652832,
      "learning_rate": 4.3189837685250534e-05,
      "loss": 4.9693,
      "step": 580
    },
    {
      "epoch": 0.4163726182074806,
      "grad_norm": 13.34694766998291,
      "learning_rate": 4.30722183015761e-05,
      "loss": 5.1653,
      "step": 590
    },
    {
      "epoch": 0.4234297812279464,
      "grad_norm": 8.879600524902344,
      "learning_rate": 4.295459891790167e-05,
      "loss": 4.7256,
      "step": 600
    },
    {
      "epoch": 0.43048694424841216,
      "grad_norm": 11.075096130371094,
      "learning_rate": 4.283697953422724e-05,
      "loss": 4.7512,
      "step": 610
    },
    {
      "epoch": 0.43754410726887794,
      "grad_norm": 10.101607322692871,
      "learning_rate": 4.271936015055281e-05,
      "loss": 4.5086,
      "step": 620
    },
    {
      "epoch": 0.4446012702893437,
      "grad_norm": 9.3810453414917,
      "learning_rate": 4.260174076687839e-05,
      "loss": 5.0921,
      "step": 630
    },
    {
      "epoch": 0.4516584333098095,
      "grad_norm": 9.491238594055176,
      "learning_rate": 4.248412138320396e-05,
      "loss": 4.5439,
      "step": 640
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 8.572540283203125,
      "learning_rate": 4.2366501999529525e-05,
      "loss": 4.7219,
      "step": 650
    },
    {
      "epoch": 0.46577275935074103,
      "grad_norm": 9.594265937805176,
      "learning_rate": 4.2248882615855096e-05,
      "loss": 4.1834,
      "step": 660
    },
    {
      "epoch": 0.4728299223712068,
      "grad_norm": 9.219919204711914,
      "learning_rate": 4.213126323218067e-05,
      "loss": 4.6037,
      "step": 670
    },
    {
      "epoch": 0.4798870853916726,
      "grad_norm": 9.437829971313477,
      "learning_rate": 4.2013643848506234e-05,
      "loss": 4.6404,
      "step": 680
    },
    {
      "epoch": 0.48694424841213835,
      "grad_norm": 9.40666389465332,
      "learning_rate": 4.1896024464831806e-05,
      "loss": 4.3007,
      "step": 690
    },
    {
      "epoch": 0.4940014114326041,
      "grad_norm": 8.271156311035156,
      "learning_rate": 4.177840508115738e-05,
      "loss": 4.985,
      "step": 700
    },
    {
      "epoch": 0.5010585744530699,
      "grad_norm": 9.671629905700684,
      "learning_rate": 4.166078569748294e-05,
      "loss": 4.6099,
      "step": 710
    },
    {
      "epoch": 0.5081157374735357,
      "grad_norm": 9.2694673538208,
      "learning_rate": 4.1543166313808515e-05,
      "loss": 4.8777,
      "step": 720
    },
    {
      "epoch": 0.5151729004940014,
      "grad_norm": 8.41503620147705,
      "learning_rate": 4.1425546930134094e-05,
      "loss": 4.5038,
      "step": 730
    },
    {
      "epoch": 0.5222300635144672,
      "grad_norm": 9.816900253295898,
      "learning_rate": 4.130792754645966e-05,
      "loss": 4.8163,
      "step": 740
    },
    {
      "epoch": 0.529287226534933,
      "grad_norm": 8.9749755859375,
      "learning_rate": 4.119030816278523e-05,
      "loss": 4.2993,
      "step": 750
    },
    {
      "epoch": 0.5363443895553988,
      "grad_norm": 10.370122909545898,
      "learning_rate": 4.10726887791108e-05,
      "loss": 4.8318,
      "step": 760
    },
    {
      "epoch": 0.5434015525758645,
      "grad_norm": 9.695919036865234,
      "learning_rate": 4.095506939543637e-05,
      "loss": 4.9918,
      "step": 770
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 9.364436149597168,
      "learning_rate": 4.083745001176194e-05,
      "loss": 4.6451,
      "step": 780
    },
    {
      "epoch": 0.5575158786167961,
      "grad_norm": 8.979764938354492,
      "learning_rate": 4.071983062808751e-05,
      "loss": 4.5565,
      "step": 790
    },
    {
      "epoch": 0.5645730416372619,
      "grad_norm": 9.374395370483398,
      "learning_rate": 4.060221124441308e-05,
      "loss": 4.6672,
      "step": 800
    },
    {
      "epoch": 0.5716302046577276,
      "grad_norm": 10.20457935333252,
      "learning_rate": 4.048459186073865e-05,
      "loss": 4.638,
      "step": 810
    },
    {
      "epoch": 0.5786873676781934,
      "grad_norm": 8.38465404510498,
      "learning_rate": 4.036697247706422e-05,
      "loss": 4.6902,
      "step": 820
    },
    {
      "epoch": 0.5857445306986592,
      "grad_norm": 10.00658130645752,
      "learning_rate": 4.024935309338979e-05,
      "loss": 4.7957,
      "step": 830
    },
    {
      "epoch": 0.592801693719125,
      "grad_norm": 9.82632827758789,
      "learning_rate": 4.0131733709715365e-05,
      "loss": 4.4547,
      "step": 840
    },
    {
      "epoch": 0.5998588567395907,
      "grad_norm": 11.756580352783203,
      "learning_rate": 4.001411432604094e-05,
      "loss": 4.9262,
      "step": 850
    },
    {
      "epoch": 0.6069160197600565,
      "grad_norm": 10.166850090026855,
      "learning_rate": 3.98964949423665e-05,
      "loss": 4.83,
      "step": 860
    },
    {
      "epoch": 0.6139731827805223,
      "grad_norm": 9.537120819091797,
      "learning_rate": 3.9778875558692074e-05,
      "loss": 5.1167,
      "step": 870
    },
    {
      "epoch": 0.621030345800988,
      "grad_norm": 8.81291389465332,
      "learning_rate": 3.9661256175017646e-05,
      "loss": 4.0962,
      "step": 880
    },
    {
      "epoch": 0.6280875088214538,
      "grad_norm": 9.698421478271484,
      "learning_rate": 3.954363679134321e-05,
      "loss": 4.7771,
      "step": 890
    },
    {
      "epoch": 0.6351446718419196,
      "grad_norm": 9.524030685424805,
      "learning_rate": 3.9426017407668783e-05,
      "loss": 4.7487,
      "step": 900
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 10.549434661865234,
      "learning_rate": 3.9308398023994355e-05,
      "loss": 4.3276,
      "step": 910
    },
    {
      "epoch": 0.6492589978828511,
      "grad_norm": 9.762537002563477,
      "learning_rate": 3.919077864031993e-05,
      "loss": 4.9132,
      "step": 920
    },
    {
      "epoch": 0.6563161609033169,
      "grad_norm": 9.407556533813477,
      "learning_rate": 3.90731592566455e-05,
      "loss": 4.4718,
      "step": 930
    },
    {
      "epoch": 0.6633733239237827,
      "grad_norm": 8.347285270690918,
      "learning_rate": 3.895553987297107e-05,
      "loss": 4.1362,
      "step": 940
    },
    {
      "epoch": 0.6704304869442484,
      "grad_norm": 9.125069618225098,
      "learning_rate": 3.8837920489296637e-05,
      "loss": 4.4408,
      "step": 950
    },
    {
      "epoch": 0.6774876499647142,
      "grad_norm": 9.320974349975586,
      "learning_rate": 3.872030110562221e-05,
      "loss": 4.6815,
      "step": 960
    },
    {
      "epoch": 0.68454481298518,
      "grad_norm": 9.82960033416748,
      "learning_rate": 3.860268172194778e-05,
      "loss": 4.8312,
      "step": 970
    },
    {
      "epoch": 0.6916019760056458,
      "grad_norm": 9.80321216583252,
      "learning_rate": 3.8485062338273346e-05,
      "loss": 4.5655,
      "step": 980
    },
    {
      "epoch": 0.6986591390261115,
      "grad_norm": 12.631752014160156,
      "learning_rate": 3.836744295459892e-05,
      "loss": 4.8426,
      "step": 990
    },
    {
      "epoch": 0.7057163020465773,
      "grad_norm": 10.28292465209961,
      "learning_rate": 3.824982357092449e-05,
      "loss": 4.6404,
      "step": 1000
    },
    {
      "epoch": 0.7127734650670431,
      "grad_norm": 9.441740989685059,
      "learning_rate": 3.813220418725006e-05,
      "loss": 4.4281,
      "step": 1010
    },
    {
      "epoch": 0.7198306280875089,
      "grad_norm": 16.977764129638672,
      "learning_rate": 3.8014584803575634e-05,
      "loss": 4.4505,
      "step": 1020
    },
    {
      "epoch": 0.7268877911079746,
      "grad_norm": 7.668999671936035,
      "learning_rate": 3.7896965419901206e-05,
      "loss": 4.6454,
      "step": 1030
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 8.546283721923828,
      "learning_rate": 3.777934603622677e-05,
      "loss": 4.678,
      "step": 1040
    },
    {
      "epoch": 0.7410021171489062,
      "grad_norm": 9.073867797851562,
      "learning_rate": 3.766172665255234e-05,
      "loss": 4.203,
      "step": 1050
    },
    {
      "epoch": 0.748059280169372,
      "grad_norm": 10.289641380310059,
      "learning_rate": 3.7544107268877915e-05,
      "loss": 4.1306,
      "step": 1060
    },
    {
      "epoch": 0.7551164431898377,
      "grad_norm": 11.478127479553223,
      "learning_rate": 3.742648788520348e-05,
      "loss": 4.4407,
      "step": 1070
    },
    {
      "epoch": 0.7621736062103035,
      "grad_norm": 9.32446575164795,
      "learning_rate": 3.730886850152905e-05,
      "loss": 4.2352,
      "step": 1080
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 8.859457969665527,
      "learning_rate": 3.7191249117854624e-05,
      "loss": 4.6367,
      "step": 1090
    },
    {
      "epoch": 0.776287932251235,
      "grad_norm": 8.488073348999023,
      "learning_rate": 3.7073629734180196e-05,
      "loss": 4.4008,
      "step": 1100
    },
    {
      "epoch": 0.7833450952717008,
      "grad_norm": 11.007185935974121,
      "learning_rate": 3.695601035050577e-05,
      "loss": 4.7841,
      "step": 1110
    },
    {
      "epoch": 0.7904022582921666,
      "grad_norm": 9.244623184204102,
      "learning_rate": 3.683839096683134e-05,
      "loss": 4.791,
      "step": 1120
    },
    {
      "epoch": 0.7974594213126324,
      "grad_norm": 9.541743278503418,
      "learning_rate": 3.6720771583156905e-05,
      "loss": 4.315,
      "step": 1130
    },
    {
      "epoch": 0.8045165843330981,
      "grad_norm": 8.969767570495605,
      "learning_rate": 3.660315219948248e-05,
      "loss": 4.3276,
      "step": 1140
    },
    {
      "epoch": 0.8115737473535639,
      "grad_norm": 9.796631813049316,
      "learning_rate": 3.648553281580805e-05,
      "loss": 4.7282,
      "step": 1150
    },
    {
      "epoch": 0.8186309103740297,
      "grad_norm": 8.522063255310059,
      "learning_rate": 3.6367913432133614e-05,
      "loss": 4.2806,
      "step": 1160
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 8.751221656799316,
      "learning_rate": 3.6250294048459186e-05,
      "loss": 4.2084,
      "step": 1170
    },
    {
      "epoch": 0.8327452364149612,
      "grad_norm": 20.337556838989258,
      "learning_rate": 3.613267466478476e-05,
      "loss": 4.1925,
      "step": 1180
    },
    {
      "epoch": 0.839802399435427,
      "grad_norm": 10.750216484069824,
      "learning_rate": 3.601505528111033e-05,
      "loss": 4.4,
      "step": 1190
    },
    {
      "epoch": 0.8468595624558928,
      "grad_norm": 9.204002380371094,
      "learning_rate": 3.58974358974359e-05,
      "loss": 4.6965,
      "step": 1200
    },
    {
      "epoch": 0.8539167254763586,
      "grad_norm": 9.752402305603027,
      "learning_rate": 3.5779816513761474e-05,
      "loss": 4.2628,
      "step": 1210
    },
    {
      "epoch": 0.8609738884968243,
      "grad_norm": 9.843485832214355,
      "learning_rate": 3.566219713008704e-05,
      "loss": 4.0312,
      "step": 1220
    },
    {
      "epoch": 0.8680310515172901,
      "grad_norm": 10.742966651916504,
      "learning_rate": 3.554457774641261e-05,
      "loss": 4.6003,
      "step": 1230
    },
    {
      "epoch": 0.8750882145377559,
      "grad_norm": 9.425711631774902,
      "learning_rate": 3.5426958362738183e-05,
      "loss": 4.6045,
      "step": 1240
    },
    {
      "epoch": 0.8821453775582216,
      "grad_norm": 9.675908088684082,
      "learning_rate": 3.530933897906375e-05,
      "loss": 4.2893,
      "step": 1250
    },
    {
      "epoch": 0.8892025405786874,
      "grad_norm": 9.927431106567383,
      "learning_rate": 3.519171959538932e-05,
      "loss": 4.4909,
      "step": 1260
    },
    {
      "epoch": 0.8962597035991532,
      "grad_norm": 8.468513488769531,
      "learning_rate": 3.507410021171489e-05,
      "loss": 4.4252,
      "step": 1270
    },
    {
      "epoch": 0.903316866619619,
      "grad_norm": 9.621182441711426,
      "learning_rate": 3.495648082804046e-05,
      "loss": 4.2707,
      "step": 1280
    },
    {
      "epoch": 0.9103740296400847,
      "grad_norm": 10.686822891235352,
      "learning_rate": 3.4838861444366037e-05,
      "loss": 4.4369,
      "step": 1290
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 10.1796293258667,
      "learning_rate": 3.472124206069161e-05,
      "loss": 4.6618,
      "step": 1300
    },
    {
      "epoch": 0.9244883556810163,
      "grad_norm": 9.444635391235352,
      "learning_rate": 3.4603622677017174e-05,
      "loss": 4.5198,
      "step": 1310
    },
    {
      "epoch": 0.9315455187014821,
      "grad_norm": 9.796131134033203,
      "learning_rate": 3.4486003293342746e-05,
      "loss": 4.4646,
      "step": 1320
    },
    {
      "epoch": 0.9386026817219478,
      "grad_norm": 10.485221862792969,
      "learning_rate": 3.436838390966832e-05,
      "loss": 4.365,
      "step": 1330
    },
    {
      "epoch": 0.9456598447424136,
      "grad_norm": 9.221111297607422,
      "learning_rate": 3.425076452599388e-05,
      "loss": 4.5638,
      "step": 1340
    },
    {
      "epoch": 0.9527170077628794,
      "grad_norm": 8.962937355041504,
      "learning_rate": 3.4133145142319455e-05,
      "loss": 4.048,
      "step": 1350
    },
    {
      "epoch": 0.9597741707833451,
      "grad_norm": 8.918802261352539,
      "learning_rate": 3.401552575864503e-05,
      "loss": 4.8599,
      "step": 1360
    },
    {
      "epoch": 0.9668313338038109,
      "grad_norm": 7.737545967102051,
      "learning_rate": 3.389790637497059e-05,
      "loss": 4.0644,
      "step": 1370
    },
    {
      "epoch": 0.9738884968242767,
      "grad_norm": 10.378456115722656,
      "learning_rate": 3.3780286991296164e-05,
      "loss": 4.5572,
      "step": 1380
    },
    {
      "epoch": 0.9809456598447425,
      "grad_norm": 9.053503036499023,
      "learning_rate": 3.366266760762174e-05,
      "loss": 4.055,
      "step": 1390
    },
    {
      "epoch": 0.9880028228652082,
      "grad_norm": 10.068299293518066,
      "learning_rate": 3.354504822394731e-05,
      "loss": 4.1076,
      "step": 1400
    },
    {
      "epoch": 0.995059985885674,
      "grad_norm": 10.465929985046387,
      "learning_rate": 3.342742884027288e-05,
      "loss": 4.6118,
      "step": 1410
    }
  ],
  "logging_steps": 10,
  "max_steps": 4251,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 92562702336000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
