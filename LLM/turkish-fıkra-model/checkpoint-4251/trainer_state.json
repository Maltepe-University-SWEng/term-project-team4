{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 4251,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007057163020465773,
      "grad_norm": 9.803939819335938,
      "learning_rate": 4.989414255469302e-05,
      "loss": 5.7008,
      "step": 10
    },
    {
      "epoch": 0.014114326040931546,
      "grad_norm": 14.286739349365234,
      "learning_rate": 4.9776523171018584e-05,
      "loss": 5.59,
      "step": 20
    },
    {
      "epoch": 0.02117148906139732,
      "grad_norm": 9.54956340789795,
      "learning_rate": 4.9658903787344156e-05,
      "loss": 5.2107,
      "step": 30
    },
    {
      "epoch": 0.028228652081863093,
      "grad_norm": 9.296425819396973,
      "learning_rate": 4.954128440366973e-05,
      "loss": 5.1773,
      "step": 40
    },
    {
      "epoch": 0.035285815102328866,
      "grad_norm": 9.932609558105469,
      "learning_rate": 4.9423665019995294e-05,
      "loss": 5.1755,
      "step": 50
    },
    {
      "epoch": 0.04234297812279464,
      "grad_norm": 9.96810245513916,
      "learning_rate": 4.9306045636320866e-05,
      "loss": 5.0035,
      "step": 60
    },
    {
      "epoch": 0.04940014114326041,
      "grad_norm": 9.972799301147461,
      "learning_rate": 4.9188426252646444e-05,
      "loss": 5.345,
      "step": 70
    },
    {
      "epoch": 0.056457304163726185,
      "grad_norm": 12.471413612365723,
      "learning_rate": 4.907080686897201e-05,
      "loss": 5.0411,
      "step": 80
    },
    {
      "epoch": 0.06351446718419196,
      "grad_norm": 9.28343677520752,
      "learning_rate": 4.895318748529758e-05,
      "loss": 5.1438,
      "step": 90
    },
    {
      "epoch": 0.07057163020465773,
      "grad_norm": 9.345253944396973,
      "learning_rate": 4.8835568101623154e-05,
      "loss": 4.9463,
      "step": 100
    },
    {
      "epoch": 0.0776287932251235,
      "grad_norm": 9.808815956115723,
      "learning_rate": 4.871794871794872e-05,
      "loss": 5.1154,
      "step": 110
    },
    {
      "epoch": 0.08468595624558928,
      "grad_norm": 9.499787330627441,
      "learning_rate": 4.860032933427429e-05,
      "loss": 4.9691,
      "step": 120
    },
    {
      "epoch": 0.09174311926605505,
      "grad_norm": 10.887115478515625,
      "learning_rate": 4.848270995059986e-05,
      "loss": 5.1977,
      "step": 130
    },
    {
      "epoch": 0.09880028228652082,
      "grad_norm": 9.585956573486328,
      "learning_rate": 4.836509056692543e-05,
      "loss": 4.8795,
      "step": 140
    },
    {
      "epoch": 0.1058574453069866,
      "grad_norm": 10.024253845214844,
      "learning_rate": 4.8247471183251e-05,
      "loss": 5.0115,
      "step": 150
    },
    {
      "epoch": 0.11291460832745237,
      "grad_norm": 10.05254077911377,
      "learning_rate": 4.812985179957657e-05,
      "loss": 5.0347,
      "step": 160
    },
    {
      "epoch": 0.11997177134791814,
      "grad_norm": 9.168549537658691,
      "learning_rate": 4.8012232415902144e-05,
      "loss": 5.2137,
      "step": 170
    },
    {
      "epoch": 0.12702893436838392,
      "grad_norm": 10.453447341918945,
      "learning_rate": 4.7894613032227716e-05,
      "loss": 5.047,
      "step": 180
    },
    {
      "epoch": 0.1340860973888497,
      "grad_norm": 8.862540245056152,
      "learning_rate": 4.777699364855329e-05,
      "loss": 5.0782,
      "step": 190
    },
    {
      "epoch": 0.14114326040931546,
      "grad_norm": 8.496728897094727,
      "learning_rate": 4.765937426487885e-05,
      "loss": 4.7925,
      "step": 200
    },
    {
      "epoch": 0.14820042342978124,
      "grad_norm": 9.522027969360352,
      "learning_rate": 4.7541754881204425e-05,
      "loss": 4.5334,
      "step": 210
    },
    {
      "epoch": 0.155257586450247,
      "grad_norm": 10.281990051269531,
      "learning_rate": 4.742413549753e-05,
      "loss": 4.8138,
      "step": 220
    },
    {
      "epoch": 0.16231474947071278,
      "grad_norm": 10.3073148727417,
      "learning_rate": 4.730651611385556e-05,
      "loss": 4.8046,
      "step": 230
    },
    {
      "epoch": 0.16937191249117856,
      "grad_norm": 9.511117935180664,
      "learning_rate": 4.7188896730181134e-05,
      "loss": 5.0472,
      "step": 240
    },
    {
      "epoch": 0.17642907551164433,
      "grad_norm": 9.618687629699707,
      "learning_rate": 4.7071277346506706e-05,
      "loss": 4.8456,
      "step": 250
    },
    {
      "epoch": 0.1834862385321101,
      "grad_norm": 8.858317375183105,
      "learning_rate": 4.695365796283228e-05,
      "loss": 4.8941,
      "step": 260
    },
    {
      "epoch": 0.19054340155257588,
      "grad_norm": 10.215332984924316,
      "learning_rate": 4.683603857915785e-05,
      "loss": 4.801,
      "step": 270
    },
    {
      "epoch": 0.19760056457304165,
      "grad_norm": 9.842853546142578,
      "learning_rate": 4.671841919548342e-05,
      "loss": 4.8424,
      "step": 280
    },
    {
      "epoch": 0.20465772759350742,
      "grad_norm": 7.9486918449401855,
      "learning_rate": 4.660079981180899e-05,
      "loss": 4.6034,
      "step": 290
    },
    {
      "epoch": 0.2117148906139732,
      "grad_norm": 10.1187744140625,
      "learning_rate": 4.648318042813456e-05,
      "loss": 4.6096,
      "step": 300
    },
    {
      "epoch": 0.21877205363443897,
      "grad_norm": 9.774480819702148,
      "learning_rate": 4.636556104446013e-05,
      "loss": 4.8018,
      "step": 310
    },
    {
      "epoch": 0.22582921665490474,
      "grad_norm": 9.91816520690918,
      "learning_rate": 4.6247941660785696e-05,
      "loss": 4.7862,
      "step": 320
    },
    {
      "epoch": 0.23288637967537051,
      "grad_norm": 9.578490257263184,
      "learning_rate": 4.613032227711127e-05,
      "loss": 4.9187,
      "step": 330
    },
    {
      "epoch": 0.2399435426958363,
      "grad_norm": 9.230071067810059,
      "learning_rate": 4.601270289343684e-05,
      "loss": 4.6422,
      "step": 340
    },
    {
      "epoch": 0.24700070571630206,
      "grad_norm": 10.706733703613281,
      "learning_rate": 4.589508350976241e-05,
      "loss": 4.7011,
      "step": 350
    },
    {
      "epoch": 0.25405786873676783,
      "grad_norm": 8.942960739135742,
      "learning_rate": 4.5777464126087984e-05,
      "loss": 4.9398,
      "step": 360
    },
    {
      "epoch": 0.2611150317572336,
      "grad_norm": 9.923922538757324,
      "learning_rate": 4.5659844742413556e-05,
      "loss": 5.1796,
      "step": 370
    },
    {
      "epoch": 0.2681721947776994,
      "grad_norm": 9.513066291809082,
      "learning_rate": 4.554222535873912e-05,
      "loss": 5.056,
      "step": 380
    },
    {
      "epoch": 0.27522935779816515,
      "grad_norm": 9.730828285217285,
      "learning_rate": 4.5424605975064694e-05,
      "loss": 5.0605,
      "step": 390
    },
    {
      "epoch": 0.2822865208186309,
      "grad_norm": 9.58809757232666,
      "learning_rate": 4.5306986591390266e-05,
      "loss": 5.0821,
      "step": 400
    },
    {
      "epoch": 0.2893436838390967,
      "grad_norm": 9.873540878295898,
      "learning_rate": 4.518936720771583e-05,
      "loss": 5.0323,
      "step": 410
    },
    {
      "epoch": 0.2964008468595625,
      "grad_norm": 9.330329895019531,
      "learning_rate": 4.50717478240414e-05,
      "loss": 4.5335,
      "step": 420
    },
    {
      "epoch": 0.30345800988002825,
      "grad_norm": 10.289673805236816,
      "learning_rate": 4.4954128440366975e-05,
      "loss": 4.9622,
      "step": 430
    },
    {
      "epoch": 0.310515172900494,
      "grad_norm": 9.374435424804688,
      "learning_rate": 4.483650905669255e-05,
      "loss": 4.788,
      "step": 440
    },
    {
      "epoch": 0.3175723359209598,
      "grad_norm": 9.762895584106445,
      "learning_rate": 4.471888967301812e-05,
      "loss": 4.9906,
      "step": 450
    },
    {
      "epoch": 0.32462949894142556,
      "grad_norm": 11.105172157287598,
      "learning_rate": 4.460127028934369e-05,
      "loss": 4.7867,
      "step": 460
    },
    {
      "epoch": 0.33168666196189134,
      "grad_norm": 9.769625663757324,
      "learning_rate": 4.4483650905669256e-05,
      "loss": 4.6602,
      "step": 470
    },
    {
      "epoch": 0.3387438249823571,
      "grad_norm": 9.89722728729248,
      "learning_rate": 4.436603152199483e-05,
      "loss": 4.5194,
      "step": 480
    },
    {
      "epoch": 0.3458009880028229,
      "grad_norm": 8.95580768585205,
      "learning_rate": 4.42484121383204e-05,
      "loss": 4.5984,
      "step": 490
    },
    {
      "epoch": 0.35285815102328866,
      "grad_norm": 9.174824714660645,
      "learning_rate": 4.4130792754645965e-05,
      "loss": 4.7109,
      "step": 500
    },
    {
      "epoch": 0.35991531404375443,
      "grad_norm": 10.257124900817871,
      "learning_rate": 4.401317337097154e-05,
      "loss": 4.7568,
      "step": 510
    },
    {
      "epoch": 0.3669724770642202,
      "grad_norm": 9.179488182067871,
      "learning_rate": 4.389555398729711e-05,
      "loss": 4.6118,
      "step": 520
    },
    {
      "epoch": 0.374029640084686,
      "grad_norm": 9.112079620361328,
      "learning_rate": 4.377793460362268e-05,
      "loss": 5.1196,
      "step": 530
    },
    {
      "epoch": 0.38108680310515175,
      "grad_norm": 9.738810539245605,
      "learning_rate": 4.366031521994825e-05,
      "loss": 4.9859,
      "step": 540
    },
    {
      "epoch": 0.3881439661256175,
      "grad_norm": 10.382537841796875,
      "learning_rate": 4.3542695836273825e-05,
      "loss": 4.7305,
      "step": 550
    },
    {
      "epoch": 0.3952011291460833,
      "grad_norm": 9.557065963745117,
      "learning_rate": 4.342507645259939e-05,
      "loss": 4.7168,
      "step": 560
    },
    {
      "epoch": 0.40225829216654907,
      "grad_norm": 10.748839378356934,
      "learning_rate": 4.330745706892496e-05,
      "loss": 4.5533,
      "step": 570
    },
    {
      "epoch": 0.40931545518701484,
      "grad_norm": 10.571599006652832,
      "learning_rate": 4.3189837685250534e-05,
      "loss": 4.9693,
      "step": 580
    },
    {
      "epoch": 0.4163726182074806,
      "grad_norm": 13.34694766998291,
      "learning_rate": 4.30722183015761e-05,
      "loss": 5.1653,
      "step": 590
    },
    {
      "epoch": 0.4234297812279464,
      "grad_norm": 8.879600524902344,
      "learning_rate": 4.295459891790167e-05,
      "loss": 4.7256,
      "step": 600
    },
    {
      "epoch": 0.43048694424841216,
      "grad_norm": 11.075096130371094,
      "learning_rate": 4.283697953422724e-05,
      "loss": 4.7512,
      "step": 610
    },
    {
      "epoch": 0.43754410726887794,
      "grad_norm": 10.101607322692871,
      "learning_rate": 4.271936015055281e-05,
      "loss": 4.5086,
      "step": 620
    },
    {
      "epoch": 0.4446012702893437,
      "grad_norm": 9.3810453414917,
      "learning_rate": 4.260174076687839e-05,
      "loss": 5.0921,
      "step": 630
    },
    {
      "epoch": 0.4516584333098095,
      "grad_norm": 9.491238594055176,
      "learning_rate": 4.248412138320396e-05,
      "loss": 4.5439,
      "step": 640
    },
    {
      "epoch": 0.45871559633027525,
      "grad_norm": 8.572540283203125,
      "learning_rate": 4.2366501999529525e-05,
      "loss": 4.7219,
      "step": 650
    },
    {
      "epoch": 0.46577275935074103,
      "grad_norm": 9.594265937805176,
      "learning_rate": 4.2248882615855096e-05,
      "loss": 4.1834,
      "step": 660
    },
    {
      "epoch": 0.4728299223712068,
      "grad_norm": 9.219919204711914,
      "learning_rate": 4.213126323218067e-05,
      "loss": 4.6037,
      "step": 670
    },
    {
      "epoch": 0.4798870853916726,
      "grad_norm": 9.437829971313477,
      "learning_rate": 4.2013643848506234e-05,
      "loss": 4.6404,
      "step": 680
    },
    {
      "epoch": 0.48694424841213835,
      "grad_norm": 9.40666389465332,
      "learning_rate": 4.1896024464831806e-05,
      "loss": 4.3007,
      "step": 690
    },
    {
      "epoch": 0.4940014114326041,
      "grad_norm": 8.271156311035156,
      "learning_rate": 4.177840508115738e-05,
      "loss": 4.985,
      "step": 700
    },
    {
      "epoch": 0.5010585744530699,
      "grad_norm": 9.671629905700684,
      "learning_rate": 4.166078569748294e-05,
      "loss": 4.6099,
      "step": 710
    },
    {
      "epoch": 0.5081157374735357,
      "grad_norm": 9.2694673538208,
      "learning_rate": 4.1543166313808515e-05,
      "loss": 4.8777,
      "step": 720
    },
    {
      "epoch": 0.5151729004940014,
      "grad_norm": 8.41503620147705,
      "learning_rate": 4.1425546930134094e-05,
      "loss": 4.5038,
      "step": 730
    },
    {
      "epoch": 0.5222300635144672,
      "grad_norm": 9.816900253295898,
      "learning_rate": 4.130792754645966e-05,
      "loss": 4.8163,
      "step": 740
    },
    {
      "epoch": 0.529287226534933,
      "grad_norm": 8.9749755859375,
      "learning_rate": 4.119030816278523e-05,
      "loss": 4.2993,
      "step": 750
    },
    {
      "epoch": 0.5363443895553988,
      "grad_norm": 10.370122909545898,
      "learning_rate": 4.10726887791108e-05,
      "loss": 4.8318,
      "step": 760
    },
    {
      "epoch": 0.5434015525758645,
      "grad_norm": 9.695919036865234,
      "learning_rate": 4.095506939543637e-05,
      "loss": 4.9918,
      "step": 770
    },
    {
      "epoch": 0.5504587155963303,
      "grad_norm": 9.364436149597168,
      "learning_rate": 4.083745001176194e-05,
      "loss": 4.6451,
      "step": 780
    },
    {
      "epoch": 0.5575158786167961,
      "grad_norm": 8.979764938354492,
      "learning_rate": 4.071983062808751e-05,
      "loss": 4.5565,
      "step": 790
    },
    {
      "epoch": 0.5645730416372619,
      "grad_norm": 9.374395370483398,
      "learning_rate": 4.060221124441308e-05,
      "loss": 4.6672,
      "step": 800
    },
    {
      "epoch": 0.5716302046577276,
      "grad_norm": 10.20457935333252,
      "learning_rate": 4.048459186073865e-05,
      "loss": 4.638,
      "step": 810
    },
    {
      "epoch": 0.5786873676781934,
      "grad_norm": 8.38465404510498,
      "learning_rate": 4.036697247706422e-05,
      "loss": 4.6902,
      "step": 820
    },
    {
      "epoch": 0.5857445306986592,
      "grad_norm": 10.00658130645752,
      "learning_rate": 4.024935309338979e-05,
      "loss": 4.7957,
      "step": 830
    },
    {
      "epoch": 0.592801693719125,
      "grad_norm": 9.82632827758789,
      "learning_rate": 4.0131733709715365e-05,
      "loss": 4.4547,
      "step": 840
    },
    {
      "epoch": 0.5998588567395907,
      "grad_norm": 11.756580352783203,
      "learning_rate": 4.001411432604094e-05,
      "loss": 4.9262,
      "step": 850
    },
    {
      "epoch": 0.6069160197600565,
      "grad_norm": 10.166850090026855,
      "learning_rate": 3.98964949423665e-05,
      "loss": 4.83,
      "step": 860
    },
    {
      "epoch": 0.6139731827805223,
      "grad_norm": 9.537120819091797,
      "learning_rate": 3.9778875558692074e-05,
      "loss": 5.1167,
      "step": 870
    },
    {
      "epoch": 0.621030345800988,
      "grad_norm": 8.81291389465332,
      "learning_rate": 3.9661256175017646e-05,
      "loss": 4.0962,
      "step": 880
    },
    {
      "epoch": 0.6280875088214538,
      "grad_norm": 9.698421478271484,
      "learning_rate": 3.954363679134321e-05,
      "loss": 4.7771,
      "step": 890
    },
    {
      "epoch": 0.6351446718419196,
      "grad_norm": 9.524030685424805,
      "learning_rate": 3.9426017407668783e-05,
      "loss": 4.7487,
      "step": 900
    },
    {
      "epoch": 0.6422018348623854,
      "grad_norm": 10.549434661865234,
      "learning_rate": 3.9308398023994355e-05,
      "loss": 4.3276,
      "step": 910
    },
    {
      "epoch": 0.6492589978828511,
      "grad_norm": 9.762537002563477,
      "learning_rate": 3.919077864031993e-05,
      "loss": 4.9132,
      "step": 920
    },
    {
      "epoch": 0.6563161609033169,
      "grad_norm": 9.407556533813477,
      "learning_rate": 3.90731592566455e-05,
      "loss": 4.4718,
      "step": 930
    },
    {
      "epoch": 0.6633733239237827,
      "grad_norm": 8.347285270690918,
      "learning_rate": 3.895553987297107e-05,
      "loss": 4.1362,
      "step": 940
    },
    {
      "epoch": 0.6704304869442484,
      "grad_norm": 9.125069618225098,
      "learning_rate": 3.8837920489296637e-05,
      "loss": 4.4408,
      "step": 950
    },
    {
      "epoch": 0.6774876499647142,
      "grad_norm": 9.320974349975586,
      "learning_rate": 3.872030110562221e-05,
      "loss": 4.6815,
      "step": 960
    },
    {
      "epoch": 0.68454481298518,
      "grad_norm": 9.82960033416748,
      "learning_rate": 3.860268172194778e-05,
      "loss": 4.8312,
      "step": 970
    },
    {
      "epoch": 0.6916019760056458,
      "grad_norm": 9.80321216583252,
      "learning_rate": 3.8485062338273346e-05,
      "loss": 4.5655,
      "step": 980
    },
    {
      "epoch": 0.6986591390261115,
      "grad_norm": 12.631752014160156,
      "learning_rate": 3.836744295459892e-05,
      "loss": 4.8426,
      "step": 990
    },
    {
      "epoch": 0.7057163020465773,
      "grad_norm": 10.28292465209961,
      "learning_rate": 3.824982357092449e-05,
      "loss": 4.6404,
      "step": 1000
    },
    {
      "epoch": 0.7127734650670431,
      "grad_norm": 9.441740989685059,
      "learning_rate": 3.813220418725006e-05,
      "loss": 4.4281,
      "step": 1010
    },
    {
      "epoch": 0.7198306280875089,
      "grad_norm": 16.977764129638672,
      "learning_rate": 3.8014584803575634e-05,
      "loss": 4.4505,
      "step": 1020
    },
    {
      "epoch": 0.7268877911079746,
      "grad_norm": 7.668999671936035,
      "learning_rate": 3.7896965419901206e-05,
      "loss": 4.6454,
      "step": 1030
    },
    {
      "epoch": 0.7339449541284404,
      "grad_norm": 8.546283721923828,
      "learning_rate": 3.777934603622677e-05,
      "loss": 4.678,
      "step": 1040
    },
    {
      "epoch": 0.7410021171489062,
      "grad_norm": 9.073867797851562,
      "learning_rate": 3.766172665255234e-05,
      "loss": 4.203,
      "step": 1050
    },
    {
      "epoch": 0.748059280169372,
      "grad_norm": 10.289641380310059,
      "learning_rate": 3.7544107268877915e-05,
      "loss": 4.1306,
      "step": 1060
    },
    {
      "epoch": 0.7551164431898377,
      "grad_norm": 11.478127479553223,
      "learning_rate": 3.742648788520348e-05,
      "loss": 4.4407,
      "step": 1070
    },
    {
      "epoch": 0.7621736062103035,
      "grad_norm": 9.32446575164795,
      "learning_rate": 3.730886850152905e-05,
      "loss": 4.2352,
      "step": 1080
    },
    {
      "epoch": 0.7692307692307693,
      "grad_norm": 8.859457969665527,
      "learning_rate": 3.7191249117854624e-05,
      "loss": 4.6367,
      "step": 1090
    },
    {
      "epoch": 0.776287932251235,
      "grad_norm": 8.488073348999023,
      "learning_rate": 3.7073629734180196e-05,
      "loss": 4.4008,
      "step": 1100
    },
    {
      "epoch": 0.7833450952717008,
      "grad_norm": 11.007185935974121,
      "learning_rate": 3.695601035050577e-05,
      "loss": 4.7841,
      "step": 1110
    },
    {
      "epoch": 0.7904022582921666,
      "grad_norm": 9.244623184204102,
      "learning_rate": 3.683839096683134e-05,
      "loss": 4.791,
      "step": 1120
    },
    {
      "epoch": 0.7974594213126324,
      "grad_norm": 9.541743278503418,
      "learning_rate": 3.6720771583156905e-05,
      "loss": 4.315,
      "step": 1130
    },
    {
      "epoch": 0.8045165843330981,
      "grad_norm": 8.969767570495605,
      "learning_rate": 3.660315219948248e-05,
      "loss": 4.3276,
      "step": 1140
    },
    {
      "epoch": 0.8115737473535639,
      "grad_norm": 9.796631813049316,
      "learning_rate": 3.648553281580805e-05,
      "loss": 4.7282,
      "step": 1150
    },
    {
      "epoch": 0.8186309103740297,
      "grad_norm": 8.522063255310059,
      "learning_rate": 3.6367913432133614e-05,
      "loss": 4.2806,
      "step": 1160
    },
    {
      "epoch": 0.8256880733944955,
      "grad_norm": 8.751221656799316,
      "learning_rate": 3.6250294048459186e-05,
      "loss": 4.2084,
      "step": 1170
    },
    {
      "epoch": 0.8327452364149612,
      "grad_norm": 20.337556838989258,
      "learning_rate": 3.613267466478476e-05,
      "loss": 4.1925,
      "step": 1180
    },
    {
      "epoch": 0.839802399435427,
      "grad_norm": 10.750216484069824,
      "learning_rate": 3.601505528111033e-05,
      "loss": 4.4,
      "step": 1190
    },
    {
      "epoch": 0.8468595624558928,
      "grad_norm": 9.204002380371094,
      "learning_rate": 3.58974358974359e-05,
      "loss": 4.6965,
      "step": 1200
    },
    {
      "epoch": 0.8539167254763586,
      "grad_norm": 9.752402305603027,
      "learning_rate": 3.5779816513761474e-05,
      "loss": 4.2628,
      "step": 1210
    },
    {
      "epoch": 0.8609738884968243,
      "grad_norm": 9.843485832214355,
      "learning_rate": 3.566219713008704e-05,
      "loss": 4.0312,
      "step": 1220
    },
    {
      "epoch": 0.8680310515172901,
      "grad_norm": 10.742966651916504,
      "learning_rate": 3.554457774641261e-05,
      "loss": 4.6003,
      "step": 1230
    },
    {
      "epoch": 0.8750882145377559,
      "grad_norm": 9.425711631774902,
      "learning_rate": 3.5426958362738183e-05,
      "loss": 4.6045,
      "step": 1240
    },
    {
      "epoch": 0.8821453775582216,
      "grad_norm": 9.675908088684082,
      "learning_rate": 3.530933897906375e-05,
      "loss": 4.2893,
      "step": 1250
    },
    {
      "epoch": 0.8892025405786874,
      "grad_norm": 9.927431106567383,
      "learning_rate": 3.519171959538932e-05,
      "loss": 4.4909,
      "step": 1260
    },
    {
      "epoch": 0.8962597035991532,
      "grad_norm": 8.468513488769531,
      "learning_rate": 3.507410021171489e-05,
      "loss": 4.4252,
      "step": 1270
    },
    {
      "epoch": 0.903316866619619,
      "grad_norm": 9.621182441711426,
      "learning_rate": 3.495648082804046e-05,
      "loss": 4.2707,
      "step": 1280
    },
    {
      "epoch": 0.9103740296400847,
      "grad_norm": 10.686822891235352,
      "learning_rate": 3.4838861444366037e-05,
      "loss": 4.4369,
      "step": 1290
    },
    {
      "epoch": 0.9174311926605505,
      "grad_norm": 10.1796293258667,
      "learning_rate": 3.472124206069161e-05,
      "loss": 4.6618,
      "step": 1300
    },
    {
      "epoch": 0.9244883556810163,
      "grad_norm": 9.444635391235352,
      "learning_rate": 3.4603622677017174e-05,
      "loss": 4.5198,
      "step": 1310
    },
    {
      "epoch": 0.9315455187014821,
      "grad_norm": 9.796131134033203,
      "learning_rate": 3.4486003293342746e-05,
      "loss": 4.4646,
      "step": 1320
    },
    {
      "epoch": 0.9386026817219478,
      "grad_norm": 10.485221862792969,
      "learning_rate": 3.436838390966832e-05,
      "loss": 4.365,
      "step": 1330
    },
    {
      "epoch": 0.9456598447424136,
      "grad_norm": 9.221111297607422,
      "learning_rate": 3.425076452599388e-05,
      "loss": 4.5638,
      "step": 1340
    },
    {
      "epoch": 0.9527170077628794,
      "grad_norm": 8.962937355041504,
      "learning_rate": 3.4133145142319455e-05,
      "loss": 4.048,
      "step": 1350
    },
    {
      "epoch": 0.9597741707833451,
      "grad_norm": 8.918802261352539,
      "learning_rate": 3.401552575864503e-05,
      "loss": 4.8599,
      "step": 1360
    },
    {
      "epoch": 0.9668313338038109,
      "grad_norm": 7.737545967102051,
      "learning_rate": 3.389790637497059e-05,
      "loss": 4.0644,
      "step": 1370
    },
    {
      "epoch": 0.9738884968242767,
      "grad_norm": 10.378456115722656,
      "learning_rate": 3.3780286991296164e-05,
      "loss": 4.5572,
      "step": 1380
    },
    {
      "epoch": 0.9809456598447425,
      "grad_norm": 9.053503036499023,
      "learning_rate": 3.366266760762174e-05,
      "loss": 4.055,
      "step": 1390
    },
    {
      "epoch": 0.9880028228652082,
      "grad_norm": 10.068299293518066,
      "learning_rate": 3.354504822394731e-05,
      "loss": 4.1076,
      "step": 1400
    },
    {
      "epoch": 0.995059985885674,
      "grad_norm": 10.465929985046387,
      "learning_rate": 3.342742884027288e-05,
      "loss": 4.6118,
      "step": 1410
    },
    {
      "epoch": 1.0021171489061398,
      "grad_norm": 8.2327299118042,
      "learning_rate": 3.330980945659845e-05,
      "loss": 3.7515,
      "step": 1420
    },
    {
      "epoch": 1.0091743119266054,
      "grad_norm": 8.136971473693848,
      "learning_rate": 3.319219007292402e-05,
      "loss": 3.0453,
      "step": 1430
    },
    {
      "epoch": 1.0162314749470713,
      "grad_norm": 9.341383934020996,
      "learning_rate": 3.307457068924959e-05,
      "loss": 3.0881,
      "step": 1440
    },
    {
      "epoch": 1.023288637967537,
      "grad_norm": 9.343704223632812,
      "learning_rate": 3.295695130557516e-05,
      "loss": 3.3557,
      "step": 1450
    },
    {
      "epoch": 1.0303458009880029,
      "grad_norm": 9.747456550598145,
      "learning_rate": 3.2839331921900726e-05,
      "loss": 3.8079,
      "step": 1460
    },
    {
      "epoch": 1.0374029640084685,
      "grad_norm": 9.489311218261719,
      "learning_rate": 3.27217125382263e-05,
      "loss": 3.0629,
      "step": 1470
    },
    {
      "epoch": 1.0444601270289344,
      "grad_norm": 8.800625801086426,
      "learning_rate": 3.260409315455187e-05,
      "loss": 3.5035,
      "step": 1480
    },
    {
      "epoch": 1.0515172900494,
      "grad_norm": 9.159561157226562,
      "learning_rate": 3.248647377087744e-05,
      "loss": 3.2981,
      "step": 1490
    },
    {
      "epoch": 1.058574453069866,
      "grad_norm": 9.634895324707031,
      "learning_rate": 3.2368854387203014e-05,
      "loss": 3.5994,
      "step": 1500
    },
    {
      "epoch": 1.0656316160903316,
      "grad_norm": 9.040266036987305,
      "learning_rate": 3.2251235003528586e-05,
      "loss": 3.2005,
      "step": 1510
    },
    {
      "epoch": 1.0726887791107975,
      "grad_norm": 9.062068939208984,
      "learning_rate": 3.213361561985415e-05,
      "loss": 3.4751,
      "step": 1520
    },
    {
      "epoch": 1.0797459421312632,
      "grad_norm": 9.769621849060059,
      "learning_rate": 3.2015996236179724e-05,
      "loss": 3.5793,
      "step": 1530
    },
    {
      "epoch": 1.086803105151729,
      "grad_norm": 9.52597427368164,
      "learning_rate": 3.1898376852505296e-05,
      "loss": 3.6133,
      "step": 1540
    },
    {
      "epoch": 1.0938602681721947,
      "grad_norm": 10.37137222290039,
      "learning_rate": 3.178075746883086e-05,
      "loss": 3.1594,
      "step": 1550
    },
    {
      "epoch": 1.1009174311926606,
      "grad_norm": 9.489562034606934,
      "learning_rate": 3.166313808515643e-05,
      "loss": 3.3398,
      "step": 1560
    },
    {
      "epoch": 1.1079745942131263,
      "grad_norm": 9.20508098602295,
      "learning_rate": 3.1545518701482005e-05,
      "loss": 3.2232,
      "step": 1570
    },
    {
      "epoch": 1.1150317572335922,
      "grad_norm": 8.731060981750488,
      "learning_rate": 3.142789931780758e-05,
      "loss": 3.4444,
      "step": 1580
    },
    {
      "epoch": 1.1220889202540578,
      "grad_norm": 9.069389343261719,
      "learning_rate": 3.131027993413315e-05,
      "loss": 3.1688,
      "step": 1590
    },
    {
      "epoch": 1.1291460832745237,
      "grad_norm": 10.623867988586426,
      "learning_rate": 3.119266055045872e-05,
      "loss": 3.2492,
      "step": 1600
    },
    {
      "epoch": 1.1362032462949894,
      "grad_norm": 9.495043754577637,
      "learning_rate": 3.1075041166784286e-05,
      "loss": 3.4973,
      "step": 1610
    },
    {
      "epoch": 1.1432604093154553,
      "grad_norm": 9.021018028259277,
      "learning_rate": 3.095742178310986e-05,
      "loss": 3.3178,
      "step": 1620
    },
    {
      "epoch": 1.150317572335921,
      "grad_norm": 7.946266174316406,
      "learning_rate": 3.083980239943543e-05,
      "loss": 3.18,
      "step": 1630
    },
    {
      "epoch": 1.1573747353563868,
      "grad_norm": 10.671736717224121,
      "learning_rate": 3.0722183015760995e-05,
      "loss": 3.265,
      "step": 1640
    },
    {
      "epoch": 1.1644318983768525,
      "grad_norm": 12.78036880493164,
      "learning_rate": 3.060456363208657e-05,
      "loss": 3.3846,
      "step": 1650
    },
    {
      "epoch": 1.1714890613973183,
      "grad_norm": 8.784818649291992,
      "learning_rate": 3.0486944248412142e-05,
      "loss": 3.1213,
      "step": 1660
    },
    {
      "epoch": 1.178546224417784,
      "grad_norm": 8.47570514678955,
      "learning_rate": 3.0369324864737708e-05,
      "loss": 3.2126,
      "step": 1670
    },
    {
      "epoch": 1.18560338743825,
      "grad_norm": 11.963545799255371,
      "learning_rate": 3.025170548106328e-05,
      "loss": 3.5155,
      "step": 1680
    },
    {
      "epoch": 1.1926605504587156,
      "grad_norm": 8.794167518615723,
      "learning_rate": 3.013408609738885e-05,
      "loss": 3.3053,
      "step": 1690
    },
    {
      "epoch": 1.1997177134791814,
      "grad_norm": 11.49416446685791,
      "learning_rate": 3.001646671371442e-05,
      "loss": 3.0822,
      "step": 1700
    },
    {
      "epoch": 1.206774876499647,
      "grad_norm": 9.592041015625,
      "learning_rate": 2.9898847330039992e-05,
      "loss": 3.3835,
      "step": 1710
    },
    {
      "epoch": 1.213832039520113,
      "grad_norm": 7.595232009887695,
      "learning_rate": 2.9781227946365564e-05,
      "loss": 2.6767,
      "step": 1720
    },
    {
      "epoch": 1.2208892025405786,
      "grad_norm": 9.92708683013916,
      "learning_rate": 2.966360856269113e-05,
      "loss": 3.5048,
      "step": 1730
    },
    {
      "epoch": 1.2279463655610445,
      "grad_norm": 7.507707595825195,
      "learning_rate": 2.9545989179016705e-05,
      "loss": 3.1566,
      "step": 1740
    },
    {
      "epoch": 1.2350035285815102,
      "grad_norm": 9.44698429107666,
      "learning_rate": 2.9428369795342277e-05,
      "loss": 3.563,
      "step": 1750
    },
    {
      "epoch": 1.242060691601976,
      "grad_norm": 9.25339412689209,
      "learning_rate": 2.9310750411667842e-05,
      "loss": 3.1339,
      "step": 1760
    },
    {
      "epoch": 1.2491178546224417,
      "grad_norm": 9.110088348388672,
      "learning_rate": 2.9193131027993414e-05,
      "loss": 3.2524,
      "step": 1770
    },
    {
      "epoch": 1.2561750176429076,
      "grad_norm": 10.379889488220215,
      "learning_rate": 2.9075511644318986e-05,
      "loss": 3.3305,
      "step": 1780
    },
    {
      "epoch": 1.2632321806633733,
      "grad_norm": 10.049041748046875,
      "learning_rate": 2.8957892260644554e-05,
      "loss": 3.27,
      "step": 1790
    },
    {
      "epoch": 1.2702893436838392,
      "grad_norm": 8.982878684997559,
      "learning_rate": 2.8840272876970126e-05,
      "loss": 2.9677,
      "step": 1800
    },
    {
      "epoch": 1.2773465067043048,
      "grad_norm": 9.689566612243652,
      "learning_rate": 2.87226534932957e-05,
      "loss": 3.4378,
      "step": 1810
    },
    {
      "epoch": 1.2844036697247707,
      "grad_norm": 11.137922286987305,
      "learning_rate": 2.8605034109621264e-05,
      "loss": 2.8093,
      "step": 1820
    },
    {
      "epoch": 1.2914608327452364,
      "grad_norm": 11.461560249328613,
      "learning_rate": 2.8487414725946836e-05,
      "loss": 3.3381,
      "step": 1830
    },
    {
      "epoch": 1.2985179957657023,
      "grad_norm": 9.8004732131958,
      "learning_rate": 2.836979534227241e-05,
      "loss": 3.3936,
      "step": 1840
    },
    {
      "epoch": 1.305575158786168,
      "grad_norm": 8.054906845092773,
      "learning_rate": 2.8252175958597976e-05,
      "loss": 3.3165,
      "step": 1850
    },
    {
      "epoch": 1.3126323218066338,
      "grad_norm": 9.269681930541992,
      "learning_rate": 2.8134556574923548e-05,
      "loss": 2.9753,
      "step": 1860
    },
    {
      "epoch": 1.3196894848270995,
      "grad_norm": 9.171138763427734,
      "learning_rate": 2.801693719124912e-05,
      "loss": 3.2608,
      "step": 1870
    },
    {
      "epoch": 1.3267466478475654,
      "grad_norm": 10.413583755493164,
      "learning_rate": 2.789931780757469e-05,
      "loss": 3.1647,
      "step": 1880
    },
    {
      "epoch": 1.333803810868031,
      "grad_norm": 8.572402954101562,
      "learning_rate": 2.778169842390026e-05,
      "loss": 3.1892,
      "step": 1890
    },
    {
      "epoch": 1.340860973888497,
      "grad_norm": 7.819873809814453,
      "learning_rate": 2.7664079040225833e-05,
      "loss": 3.1809,
      "step": 1900
    },
    {
      "epoch": 1.3479181369089626,
      "grad_norm": 12.187178611755371,
      "learning_rate": 2.7546459656551398e-05,
      "loss": 3.6215,
      "step": 1910
    },
    {
      "epoch": 1.3549752999294284,
      "grad_norm": 9.430079460144043,
      "learning_rate": 2.742884027287697e-05,
      "loss": 3.1414,
      "step": 1920
    },
    {
      "epoch": 1.362032462949894,
      "grad_norm": 7.060177803039551,
      "learning_rate": 2.7311220889202542e-05,
      "loss": 2.9197,
      "step": 1930
    },
    {
      "epoch": 1.36908962597036,
      "grad_norm": 7.912906646728516,
      "learning_rate": 2.719360150552811e-05,
      "loss": 3.3192,
      "step": 1940
    },
    {
      "epoch": 1.3761467889908257,
      "grad_norm": 11.055102348327637,
      "learning_rate": 2.7075982121853682e-05,
      "loss": 3.3206,
      "step": 1950
    },
    {
      "epoch": 1.3832039520112915,
      "grad_norm": 10.177406311035156,
      "learning_rate": 2.6958362738179254e-05,
      "loss": 3.3881,
      "step": 1960
    },
    {
      "epoch": 1.3902611150317572,
      "grad_norm": 8.931294441223145,
      "learning_rate": 2.6840743354504823e-05,
      "loss": 3.1955,
      "step": 1970
    },
    {
      "epoch": 1.397318278052223,
      "grad_norm": 9.865416526794434,
      "learning_rate": 2.6723123970830395e-05,
      "loss": 3.37,
      "step": 1980
    },
    {
      "epoch": 1.4043754410726887,
      "grad_norm": 7.6132659912109375,
      "learning_rate": 2.6605504587155967e-05,
      "loss": 3.2912,
      "step": 1990
    },
    {
      "epoch": 1.4114326040931546,
      "grad_norm": 10.367283821105957,
      "learning_rate": 2.6487885203481532e-05,
      "loss": 3.445,
      "step": 2000
    },
    {
      "epoch": 1.4184897671136203,
      "grad_norm": 10.666753768920898,
      "learning_rate": 2.6370265819807104e-05,
      "loss": 3.4661,
      "step": 2010
    },
    {
      "epoch": 1.4255469301340862,
      "grad_norm": 8.128179550170898,
      "learning_rate": 2.6252646436132676e-05,
      "loss": 2.7361,
      "step": 2020
    },
    {
      "epoch": 1.4326040931545518,
      "grad_norm": 7.9328179359436035,
      "learning_rate": 2.6135027052458245e-05,
      "loss": 3.2388,
      "step": 2030
    },
    {
      "epoch": 1.4396612561750177,
      "grad_norm": 8.83342170715332,
      "learning_rate": 2.6017407668783817e-05,
      "loss": 3.0454,
      "step": 2040
    },
    {
      "epoch": 1.4467184191954834,
      "grad_norm": 9.105942726135254,
      "learning_rate": 2.589978828510939e-05,
      "loss": 3.1927,
      "step": 2050
    },
    {
      "epoch": 1.4537755822159493,
      "grad_norm": 11.104310989379883,
      "learning_rate": 2.5782168901434954e-05,
      "loss": 3.0924,
      "step": 2060
    },
    {
      "epoch": 1.460832745236415,
      "grad_norm": 9.372265815734863,
      "learning_rate": 2.566454951776053e-05,
      "loss": 3.1173,
      "step": 2070
    },
    {
      "epoch": 1.4678899082568808,
      "grad_norm": 9.45535945892334,
      "learning_rate": 2.55469301340861e-05,
      "loss": 3.6022,
      "step": 2080
    },
    {
      "epoch": 1.4749470712773465,
      "grad_norm": 10.982789039611816,
      "learning_rate": 2.5429310750411667e-05,
      "loss": 3.1189,
      "step": 2090
    },
    {
      "epoch": 1.4820042342978124,
      "grad_norm": 11.055499076843262,
      "learning_rate": 2.531169136673724e-05,
      "loss": 3.2016,
      "step": 2100
    },
    {
      "epoch": 1.489061397318278,
      "grad_norm": 8.667793273925781,
      "learning_rate": 2.519407198306281e-05,
      "loss": 3.0477,
      "step": 2110
    },
    {
      "epoch": 1.496118560338744,
      "grad_norm": 10.937450408935547,
      "learning_rate": 2.507645259938838e-05,
      "loss": 3.5558,
      "step": 2120
    },
    {
      "epoch": 1.5031757233592096,
      "grad_norm": 12.143418312072754,
      "learning_rate": 2.495883321571395e-05,
      "loss": 3.4247,
      "step": 2130
    },
    {
      "epoch": 1.5102328863796752,
      "grad_norm": 9.441715240478516,
      "learning_rate": 2.4841213832039523e-05,
      "loss": 3.054,
      "step": 2140
    },
    {
      "epoch": 1.5172900494001411,
      "grad_norm": 10.67164134979248,
      "learning_rate": 2.472359444836509e-05,
      "loss": 3.2803,
      "step": 2150
    },
    {
      "epoch": 1.524347212420607,
      "grad_norm": 9.373043060302734,
      "learning_rate": 2.460597506469066e-05,
      "loss": 3.0506,
      "step": 2160
    },
    {
      "epoch": 1.5314043754410727,
      "grad_norm": 10.193906784057617,
      "learning_rate": 2.4488355681016236e-05,
      "loss": 3.2196,
      "step": 2170
    },
    {
      "epoch": 1.5384615384615383,
      "grad_norm": 11.146809577941895,
      "learning_rate": 2.4370736297341804e-05,
      "loss": 2.9717,
      "step": 2180
    },
    {
      "epoch": 1.5455187014820042,
      "grad_norm": 8.299763679504395,
      "learning_rate": 2.4253116913667373e-05,
      "loss": 2.9496,
      "step": 2190
    },
    {
      "epoch": 1.55257586450247,
      "grad_norm": 10.81899356842041,
      "learning_rate": 2.4135497529992945e-05,
      "loss": 3.0303,
      "step": 2200
    },
    {
      "epoch": 1.5596330275229358,
      "grad_norm": 8.924066543579102,
      "learning_rate": 2.4017878146318513e-05,
      "loss": 3.1408,
      "step": 2210
    },
    {
      "epoch": 1.5666901905434014,
      "grad_norm": 9.722600936889648,
      "learning_rate": 2.3900258762644085e-05,
      "loss": 3.6472,
      "step": 2220
    },
    {
      "epoch": 1.5737473535638673,
      "grad_norm": 8.678665161132812,
      "learning_rate": 2.3782639378969657e-05,
      "loss": 2.9751,
      "step": 2230
    },
    {
      "epoch": 1.5808045165843332,
      "grad_norm": 10.651862144470215,
      "learning_rate": 2.3665019995295226e-05,
      "loss": 3.1346,
      "step": 2240
    },
    {
      "epoch": 1.5878616796047988,
      "grad_norm": 12.816217422485352,
      "learning_rate": 2.3547400611620795e-05,
      "loss": 3.4783,
      "step": 2250
    },
    {
      "epoch": 1.5949188426252645,
      "grad_norm": 9.408286094665527,
      "learning_rate": 2.3429781227946367e-05,
      "loss": 3.6993,
      "step": 2260
    },
    {
      "epoch": 1.6019760056457304,
      "grad_norm": 9.457021713256836,
      "learning_rate": 2.331216184427194e-05,
      "loss": 3.3538,
      "step": 2270
    },
    {
      "epoch": 1.6090331686661963,
      "grad_norm": 9.672845840454102,
      "learning_rate": 2.3194542460597507e-05,
      "loss": 3.0079,
      "step": 2280
    },
    {
      "epoch": 1.616090331686662,
      "grad_norm": 8.692377090454102,
      "learning_rate": 2.307692307692308e-05,
      "loss": 3.5226,
      "step": 2290
    },
    {
      "epoch": 1.6231474947071276,
      "grad_norm": 10.768632888793945,
      "learning_rate": 2.2959303693248648e-05,
      "loss": 3.4396,
      "step": 2300
    },
    {
      "epoch": 1.6302046577275935,
      "grad_norm": 11.301620483398438,
      "learning_rate": 2.284168430957422e-05,
      "loss": 3.5437,
      "step": 2310
    },
    {
      "epoch": 1.6372618207480594,
      "grad_norm": 9.935906410217285,
      "learning_rate": 2.272406492589979e-05,
      "loss": 2.8911,
      "step": 2320
    },
    {
      "epoch": 1.644318983768525,
      "grad_norm": 8.616249084472656,
      "learning_rate": 2.260644554222536e-05,
      "loss": 2.6739,
      "step": 2330
    },
    {
      "epoch": 1.6513761467889907,
      "grad_norm": 9.840902328491211,
      "learning_rate": 2.248882615855093e-05,
      "loss": 3.3054,
      "step": 2340
    },
    {
      "epoch": 1.6584333098094566,
      "grad_norm": 10.59205150604248,
      "learning_rate": 2.23712067748765e-05,
      "loss": 3.3512,
      "step": 2350
    },
    {
      "epoch": 1.6654904728299225,
      "grad_norm": 8.032137870788574,
      "learning_rate": 2.2253587391202073e-05,
      "loss": 3.1585,
      "step": 2360
    },
    {
      "epoch": 1.6725476358503881,
      "grad_norm": 9.984152793884277,
      "learning_rate": 2.213596800752764e-05,
      "loss": 3.3384,
      "step": 2370
    },
    {
      "epoch": 1.6796047988708538,
      "grad_norm": 9.333093643188477,
      "learning_rate": 2.2018348623853213e-05,
      "loss": 3.1832,
      "step": 2380
    },
    {
      "epoch": 1.6866619618913197,
      "grad_norm": 8.905866622924805,
      "learning_rate": 2.1900729240178782e-05,
      "loss": 3.0662,
      "step": 2390
    },
    {
      "epoch": 1.6937191249117856,
      "grad_norm": 10.644828796386719,
      "learning_rate": 2.1783109856504354e-05,
      "loss": 3.1313,
      "step": 2400
    },
    {
      "epoch": 1.7007762879322512,
      "grad_norm": 10.560699462890625,
      "learning_rate": 2.1665490472829926e-05,
      "loss": 2.8501,
      "step": 2410
    },
    {
      "epoch": 1.7078334509527169,
      "grad_norm": 11.571292877197266,
      "learning_rate": 2.1547871089155495e-05,
      "loss": 3.6606,
      "step": 2420
    },
    {
      "epoch": 1.7148906139731828,
      "grad_norm": 10.100242614746094,
      "learning_rate": 2.1430251705481063e-05,
      "loss": 2.8483,
      "step": 2430
    },
    {
      "epoch": 1.7219477769936486,
      "grad_norm": 7.932751655578613,
      "learning_rate": 2.1312632321806635e-05,
      "loss": 2.5609,
      "step": 2440
    },
    {
      "epoch": 1.7290049400141143,
      "grad_norm": 9.209373474121094,
      "learning_rate": 2.1195012938132207e-05,
      "loss": 2.4713,
      "step": 2450
    },
    {
      "epoch": 1.73606210303458,
      "grad_norm": 10.494736671447754,
      "learning_rate": 2.1077393554457776e-05,
      "loss": 2.54,
      "step": 2460
    },
    {
      "epoch": 1.7431192660550459,
      "grad_norm": 11.318928718566895,
      "learning_rate": 2.0959774170783348e-05,
      "loss": 3.5764,
      "step": 2470
    },
    {
      "epoch": 1.7501764290755117,
      "grad_norm": 8.998852729797363,
      "learning_rate": 2.0842154787108916e-05,
      "loss": 3.5373,
      "step": 2480
    },
    {
      "epoch": 1.7572335920959774,
      "grad_norm": 9.168340682983398,
      "learning_rate": 2.0724535403434485e-05,
      "loss": 3.3622,
      "step": 2490
    },
    {
      "epoch": 1.764290755116443,
      "grad_norm": 7.895203590393066,
      "learning_rate": 2.060691601976006e-05,
      "loss": 2.7737,
      "step": 2500
    },
    {
      "epoch": 1.771347918136909,
      "grad_norm": 8.677999496459961,
      "learning_rate": 2.048929663608563e-05,
      "loss": 3.2876,
      "step": 2510
    },
    {
      "epoch": 1.7784050811573748,
      "grad_norm": 8.814691543579102,
      "learning_rate": 2.0371677252411197e-05,
      "loss": 2.7338,
      "step": 2520
    },
    {
      "epoch": 1.7854622441778405,
      "grad_norm": 8.987536430358887,
      "learning_rate": 2.025405786873677e-05,
      "loss": 3.3134,
      "step": 2530
    },
    {
      "epoch": 1.7925194071983062,
      "grad_norm": 9.056525230407715,
      "learning_rate": 2.0136438485062338e-05,
      "loss": 3.2215,
      "step": 2540
    },
    {
      "epoch": 1.799576570218772,
      "grad_norm": 7.500690460205078,
      "learning_rate": 2.001881910138791e-05,
      "loss": 3.0919,
      "step": 2550
    },
    {
      "epoch": 1.806633733239238,
      "grad_norm": 11.403131484985352,
      "learning_rate": 1.9901199717713482e-05,
      "loss": 3.3601,
      "step": 2560
    },
    {
      "epoch": 1.8136908962597036,
      "grad_norm": 10.890182495117188,
      "learning_rate": 1.978358033403905e-05,
      "loss": 3.5398,
      "step": 2570
    },
    {
      "epoch": 1.8207480592801693,
      "grad_norm": 7.920186996459961,
      "learning_rate": 1.966596095036462e-05,
      "loss": 3.1623,
      "step": 2580
    },
    {
      "epoch": 1.8278052223006351,
      "grad_norm": 8.668757438659668,
      "learning_rate": 1.954834156669019e-05,
      "loss": 3.0334,
      "step": 2590
    },
    {
      "epoch": 1.834862385321101,
      "grad_norm": 7.825656414031982,
      "learning_rate": 1.9430722183015763e-05,
      "loss": 2.7953,
      "step": 2600
    },
    {
      "epoch": 1.8419195483415667,
      "grad_norm": 10.514918327331543,
      "learning_rate": 1.9313102799341332e-05,
      "loss": 2.9482,
      "step": 2610
    },
    {
      "epoch": 1.8489767113620323,
      "grad_norm": 10.487438201904297,
      "learning_rate": 1.9195483415666904e-05,
      "loss": 3.518,
      "step": 2620
    },
    {
      "epoch": 1.8560338743824982,
      "grad_norm": 8.738203048706055,
      "learning_rate": 1.9077864031992472e-05,
      "loss": 3.2263,
      "step": 2630
    },
    {
      "epoch": 1.8630910374029641,
      "grad_norm": 9.8330078125,
      "learning_rate": 1.8960244648318044e-05,
      "loss": 3.3114,
      "step": 2640
    },
    {
      "epoch": 1.8701482004234298,
      "grad_norm": 10.00748348236084,
      "learning_rate": 1.8842625264643616e-05,
      "loss": 3.2407,
      "step": 2650
    },
    {
      "epoch": 1.8772053634438954,
      "grad_norm": 9.536447525024414,
      "learning_rate": 1.8725005880969185e-05,
      "loss": 2.9083,
      "step": 2660
    },
    {
      "epoch": 1.8842625264643613,
      "grad_norm": 11.740077018737793,
      "learning_rate": 1.8607386497294753e-05,
      "loss": 3.5827,
      "step": 2670
    },
    {
      "epoch": 1.8913196894848272,
      "grad_norm": 9.999406814575195,
      "learning_rate": 1.8489767113620325e-05,
      "loss": 3.3498,
      "step": 2680
    },
    {
      "epoch": 1.8983768525052929,
      "grad_norm": 9.886404037475586,
      "learning_rate": 1.8372147729945897e-05,
      "loss": 3.3939,
      "step": 2690
    },
    {
      "epoch": 1.9054340155257585,
      "grad_norm": 10.254223823547363,
      "learning_rate": 1.8254528346271466e-05,
      "loss": 3.2577,
      "step": 2700
    },
    {
      "epoch": 1.9124911785462244,
      "grad_norm": 10.898235321044922,
      "learning_rate": 1.8136908962597038e-05,
      "loss": 3.2139,
      "step": 2710
    },
    {
      "epoch": 1.9195483415666903,
      "grad_norm": 9.844694137573242,
      "learning_rate": 1.8019289578922607e-05,
      "loss": 3.2766,
      "step": 2720
    },
    {
      "epoch": 1.926605504587156,
      "grad_norm": 11.458808898925781,
      "learning_rate": 1.790167019524818e-05,
      "loss": 3.1878,
      "step": 2730
    },
    {
      "epoch": 1.9336626676076216,
      "grad_norm": 8.555170059204102,
      "learning_rate": 1.778405081157375e-05,
      "loss": 3.4203,
      "step": 2740
    },
    {
      "epoch": 1.9407198306280875,
      "grad_norm": 8.478745460510254,
      "learning_rate": 1.766643142789932e-05,
      "loss": 3.1935,
      "step": 2750
    },
    {
      "epoch": 1.9477769936485534,
      "grad_norm": 8.36948013305664,
      "learning_rate": 1.7548812044224888e-05,
      "loss": 3.1638,
      "step": 2760
    },
    {
      "epoch": 1.954834156669019,
      "grad_norm": 9.765765190124512,
      "learning_rate": 1.743119266055046e-05,
      "loss": 3.3895,
      "step": 2770
    },
    {
      "epoch": 1.9618913196894847,
      "grad_norm": 8.208702087402344,
      "learning_rate": 1.7313573276876032e-05,
      "loss": 3.1535,
      "step": 2780
    },
    {
      "epoch": 1.9689484827099506,
      "grad_norm": 10.946487426757812,
      "learning_rate": 1.71959538932016e-05,
      "loss": 3.8771,
      "step": 2790
    },
    {
      "epoch": 1.9760056457304165,
      "grad_norm": 11.11671257019043,
      "learning_rate": 1.7078334509527172e-05,
      "loss": 2.9497,
      "step": 2800
    },
    {
      "epoch": 1.9830628087508821,
      "grad_norm": 7.591213703155518,
      "learning_rate": 1.696071512585274e-05,
      "loss": 2.7794,
      "step": 2810
    },
    {
      "epoch": 1.9901199717713478,
      "grad_norm": 7.497946739196777,
      "learning_rate": 1.684309574217831e-05,
      "loss": 2.7743,
      "step": 2820
    },
    {
      "epoch": 1.9971771347918137,
      "grad_norm": 9.849123001098633,
      "learning_rate": 1.6725476358503885e-05,
      "loss": 3.5489,
      "step": 2830
    },
    {
      "epoch": 2.0042342978122796,
      "grad_norm": 8.308844566345215,
      "learning_rate": 1.6607856974829453e-05,
      "loss": 2.8291,
      "step": 2840
    },
    {
      "epoch": 2.0112914608327452,
      "grad_norm": 10.006265640258789,
      "learning_rate": 1.6490237591155022e-05,
      "loss": 2.44,
      "step": 2850
    },
    {
      "epoch": 2.018348623853211,
      "grad_norm": 9.45394229888916,
      "learning_rate": 1.6372618207480594e-05,
      "loss": 2.8004,
      "step": 2860
    },
    {
      "epoch": 2.0254057868736766,
      "grad_norm": 7.853753566741943,
      "learning_rate": 1.6254998823806163e-05,
      "loss": 2.5688,
      "step": 2870
    },
    {
      "epoch": 2.0324629498941427,
      "grad_norm": 8.969048500061035,
      "learning_rate": 1.6137379440131735e-05,
      "loss": 2.5704,
      "step": 2880
    },
    {
      "epoch": 2.0395201129146083,
      "grad_norm": 7.87722635269165,
      "learning_rate": 1.6019760056457307e-05,
      "loss": 2.5603,
      "step": 2890
    },
    {
      "epoch": 2.046577275935074,
      "grad_norm": 10.326420783996582,
      "learning_rate": 1.5902140672782875e-05,
      "loss": 2.8301,
      "step": 2900
    },
    {
      "epoch": 2.05363443895554,
      "grad_norm": 7.7150421142578125,
      "learning_rate": 1.5784521289108444e-05,
      "loss": 2.3697,
      "step": 2910
    },
    {
      "epoch": 2.0606916019760058,
      "grad_norm": 10.814008712768555,
      "learning_rate": 1.5666901905434016e-05,
      "loss": 2.2874,
      "step": 2920
    },
    {
      "epoch": 2.0677487649964714,
      "grad_norm": 10.132932662963867,
      "learning_rate": 1.5549282521759588e-05,
      "loss": 2.5094,
      "step": 2930
    },
    {
      "epoch": 2.074805928016937,
      "grad_norm": 10.262823104858398,
      "learning_rate": 1.5431663138085156e-05,
      "loss": 2.7845,
      "step": 2940
    },
    {
      "epoch": 2.0818630910374027,
      "grad_norm": 7.6863322257995605,
      "learning_rate": 1.531404375441073e-05,
      "loss": 2.466,
      "step": 2950
    },
    {
      "epoch": 2.088920254057869,
      "grad_norm": 10.477224349975586,
      "learning_rate": 1.5196424370736299e-05,
      "loss": 2.5886,
      "step": 2960
    },
    {
      "epoch": 2.0959774170783345,
      "grad_norm": 7.716808319091797,
      "learning_rate": 1.5078804987061867e-05,
      "loss": 2.1919,
      "step": 2970
    },
    {
      "epoch": 2.1030345800988,
      "grad_norm": 14.137833595275879,
      "learning_rate": 1.496118560338744e-05,
      "loss": 2.525,
      "step": 2980
    },
    {
      "epoch": 2.1100917431192663,
      "grad_norm": 9.822708129882812,
      "learning_rate": 1.484356621971301e-05,
      "loss": 2.1646,
      "step": 2990
    },
    {
      "epoch": 2.117148906139732,
      "grad_norm": 9.448744773864746,
      "learning_rate": 1.4725946836038578e-05,
      "loss": 2.6059,
      "step": 3000
    },
    {
      "epoch": 2.1242060691601976,
      "grad_norm": 7.090336322784424,
      "learning_rate": 1.4608327452364152e-05,
      "loss": 2.3994,
      "step": 3010
    },
    {
      "epoch": 2.1312632321806633,
      "grad_norm": 8.049150466918945,
      "learning_rate": 1.449070806868972e-05,
      "loss": 2.3666,
      "step": 3020
    },
    {
      "epoch": 2.138320395201129,
      "grad_norm": 11.178055763244629,
      "learning_rate": 1.437308868501529e-05,
      "loss": 2.1527,
      "step": 3030
    },
    {
      "epoch": 2.145377558221595,
      "grad_norm": 8.993290901184082,
      "learning_rate": 1.4255469301340863e-05,
      "loss": 2.5538,
      "step": 3040
    },
    {
      "epoch": 2.1524347212420607,
      "grad_norm": 11.030892372131348,
      "learning_rate": 1.4137849917666431e-05,
      "loss": 2.5082,
      "step": 3050
    },
    {
      "epoch": 2.1594918842625264,
      "grad_norm": 9.07384204864502,
      "learning_rate": 1.4020230533992002e-05,
      "loss": 2.215,
      "step": 3060
    },
    {
      "epoch": 2.1665490472829925,
      "grad_norm": 8.508907318115234,
      "learning_rate": 1.3902611150317574e-05,
      "loss": 2.6297,
      "step": 3070
    },
    {
      "epoch": 2.173606210303458,
      "grad_norm": 9.010641098022461,
      "learning_rate": 1.3784991766643144e-05,
      "loss": 2.1913,
      "step": 3080
    },
    {
      "epoch": 2.180663373323924,
      "grad_norm": 9.113221168518066,
      "learning_rate": 1.3667372382968712e-05,
      "loss": 2.7471,
      "step": 3090
    },
    {
      "epoch": 2.1877205363443895,
      "grad_norm": 9.998597145080566,
      "learning_rate": 1.3549752999294284e-05,
      "loss": 2.1174,
      "step": 3100
    },
    {
      "epoch": 2.194777699364855,
      "grad_norm": 9.163955688476562,
      "learning_rate": 1.3432133615619855e-05,
      "loss": 2.0601,
      "step": 3110
    },
    {
      "epoch": 2.2018348623853212,
      "grad_norm": 9.6165132522583,
      "learning_rate": 1.3314514231945425e-05,
      "loss": 2.4398,
      "step": 3120
    },
    {
      "epoch": 2.208892025405787,
      "grad_norm": 10.003418922424316,
      "learning_rate": 1.3196894848270997e-05,
      "loss": 2.5688,
      "step": 3130
    },
    {
      "epoch": 2.2159491884262525,
      "grad_norm": 11.153634071350098,
      "learning_rate": 1.3079275464596566e-05,
      "loss": 2.8897,
      "step": 3140
    },
    {
      "epoch": 2.2230063514467187,
      "grad_norm": 10.785795211791992,
      "learning_rate": 1.2961656080922136e-05,
      "loss": 2.4894,
      "step": 3150
    },
    {
      "epoch": 2.2300635144671843,
      "grad_norm": 9.398679733276367,
      "learning_rate": 1.2844036697247708e-05,
      "loss": 2.4437,
      "step": 3160
    },
    {
      "epoch": 2.23712067748765,
      "grad_norm": 10.294021606445312,
      "learning_rate": 1.2726417313573278e-05,
      "loss": 2.4469,
      "step": 3170
    },
    {
      "epoch": 2.2441778405081156,
      "grad_norm": 11.386919975280762,
      "learning_rate": 1.2608797929898847e-05,
      "loss": 2.3704,
      "step": 3180
    },
    {
      "epoch": 2.2512350035285813,
      "grad_norm": 9.682337760925293,
      "learning_rate": 1.2491178546224419e-05,
      "loss": 2.2285,
      "step": 3190
    },
    {
      "epoch": 2.2582921665490474,
      "grad_norm": 9.960783958435059,
      "learning_rate": 1.2373559162549989e-05,
      "loss": 2.3401,
      "step": 3200
    },
    {
      "epoch": 2.265349329569513,
      "grad_norm": 7.221277236938477,
      "learning_rate": 1.225593977887556e-05,
      "loss": 2.0099,
      "step": 3210
    },
    {
      "epoch": 2.2724064925899787,
      "grad_norm": 9.057502746582031,
      "learning_rate": 1.2138320395201131e-05,
      "loss": 2.6478,
      "step": 3220
    },
    {
      "epoch": 2.279463655610445,
      "grad_norm": 9.831192016601562,
      "learning_rate": 1.20207010115267e-05,
      "loss": 2.432,
      "step": 3230
    },
    {
      "epoch": 2.2865208186309105,
      "grad_norm": 7.893007755279541,
      "learning_rate": 1.190308162785227e-05,
      "loss": 2.5548,
      "step": 3240
    },
    {
      "epoch": 2.293577981651376,
      "grad_norm": 10.531278610229492,
      "learning_rate": 1.1785462244177842e-05,
      "loss": 2.336,
      "step": 3250
    },
    {
      "epoch": 2.300635144671842,
      "grad_norm": 6.436647415161133,
      "learning_rate": 1.166784286050341e-05,
      "loss": 2.3925,
      "step": 3260
    },
    {
      "epoch": 2.3076923076923075,
      "grad_norm": 10.265477180480957,
      "learning_rate": 1.1550223476828983e-05,
      "loss": 2.2422,
      "step": 3270
    },
    {
      "epoch": 2.3147494707127736,
      "grad_norm": 11.121377944946289,
      "learning_rate": 1.1432604093154553e-05,
      "loss": 2.7258,
      "step": 3280
    },
    {
      "epoch": 2.3218066337332393,
      "grad_norm": 7.572342395782471,
      "learning_rate": 1.1314984709480123e-05,
      "loss": 2.1132,
      "step": 3290
    },
    {
      "epoch": 2.328863796753705,
      "grad_norm": 8.346359252929688,
      "learning_rate": 1.1197365325805694e-05,
      "loss": 2.4205,
      "step": 3300
    },
    {
      "epoch": 2.335920959774171,
      "grad_norm": 11.14063835144043,
      "learning_rate": 1.1079745942131264e-05,
      "loss": 2.5568,
      "step": 3310
    },
    {
      "epoch": 2.3429781227946367,
      "grad_norm": 10.735372543334961,
      "learning_rate": 1.0962126558456834e-05,
      "loss": 2.414,
      "step": 3320
    },
    {
      "epoch": 2.3500352858151023,
      "grad_norm": 7.0903849601745605,
      "learning_rate": 1.0844507174782404e-05,
      "loss": 2.2401,
      "step": 3330
    },
    {
      "epoch": 2.357092448835568,
      "grad_norm": 8.95927619934082,
      "learning_rate": 1.0726887791107976e-05,
      "loss": 2.5676,
      "step": 3340
    },
    {
      "epoch": 2.3641496118560337,
      "grad_norm": 9.369848251342773,
      "learning_rate": 1.0609268407433545e-05,
      "loss": 2.6601,
      "step": 3350
    },
    {
      "epoch": 2.3712067748765,
      "grad_norm": 10.339611053466797,
      "learning_rate": 1.0491649023759115e-05,
      "loss": 2.6865,
      "step": 3360
    },
    {
      "epoch": 2.3782639378969654,
      "grad_norm": 8.38386344909668,
      "learning_rate": 1.0374029640084687e-05,
      "loss": 2.466,
      "step": 3370
    },
    {
      "epoch": 2.385321100917431,
      "grad_norm": 8.128676414489746,
      "learning_rate": 1.0256410256410256e-05,
      "loss": 1.9467,
      "step": 3380
    },
    {
      "epoch": 2.392378263937897,
      "grad_norm": 11.766846656799316,
      "learning_rate": 1.0138790872735828e-05,
      "loss": 2.5717,
      "step": 3390
    },
    {
      "epoch": 2.399435426958363,
      "grad_norm": 8.919745445251465,
      "learning_rate": 1.0021171489061398e-05,
      "loss": 2.7722,
      "step": 3400
    },
    {
      "epoch": 2.4064925899788285,
      "grad_norm": 8.285553932189941,
      "learning_rate": 9.903552105386968e-06,
      "loss": 2.5069,
      "step": 3410
    },
    {
      "epoch": 2.413549752999294,
      "grad_norm": 9.916200637817383,
      "learning_rate": 9.785932721712539e-06,
      "loss": 2.5367,
      "step": 3420
    },
    {
      "epoch": 2.42060691601976,
      "grad_norm": 10.263566970825195,
      "learning_rate": 9.668313338038109e-06,
      "loss": 2.9201,
      "step": 3430
    },
    {
      "epoch": 2.427664079040226,
      "grad_norm": 7.99003267288208,
      "learning_rate": 9.55069395436368e-06,
      "loss": 2.1074,
      "step": 3440
    },
    {
      "epoch": 2.4347212420606916,
      "grad_norm": 10.108152389526367,
      "learning_rate": 9.43307457068925e-06,
      "loss": 2.5158,
      "step": 3450
    },
    {
      "epoch": 2.4417784050811573,
      "grad_norm": 11.973586082458496,
      "learning_rate": 9.315455187014822e-06,
      "loss": 2.877,
      "step": 3460
    },
    {
      "epoch": 2.4488355681016234,
      "grad_norm": 11.214147567749023,
      "learning_rate": 9.19783580334039e-06,
      "loss": 2.6075,
      "step": 3470
    },
    {
      "epoch": 2.455892731122089,
      "grad_norm": 6.8856072425842285,
      "learning_rate": 9.080216419665962e-06,
      "loss": 2.6208,
      "step": 3480
    },
    {
      "epoch": 2.4629498941425547,
      "grad_norm": 10.004951477050781,
      "learning_rate": 8.962597035991532e-06,
      "loss": 2.5759,
      "step": 3490
    },
    {
      "epoch": 2.4700070571630204,
      "grad_norm": 7.225378036499023,
      "learning_rate": 8.844977652317101e-06,
      "loss": 2.1512,
      "step": 3500
    },
    {
      "epoch": 2.477064220183486,
      "grad_norm": 11.088188171386719,
      "learning_rate": 8.727358268642673e-06,
      "loss": 2.6078,
      "step": 3510
    },
    {
      "epoch": 2.484121383203952,
      "grad_norm": 10.031007766723633,
      "learning_rate": 8.609738884968243e-06,
      "loss": 2.4956,
      "step": 3520
    },
    {
      "epoch": 2.491178546224418,
      "grad_norm": 7.9236602783203125,
      "learning_rate": 8.492119501293814e-06,
      "loss": 2.3078,
      "step": 3530
    },
    {
      "epoch": 2.4982357092448835,
      "grad_norm": 11.009664535522461,
      "learning_rate": 8.374500117619384e-06,
      "loss": 2.5931,
      "step": 3540
    },
    {
      "epoch": 2.5052928722653496,
      "grad_norm": 11.020478248596191,
      "learning_rate": 8.256880733944954e-06,
      "loss": 2.0743,
      "step": 3550
    },
    {
      "epoch": 2.5123500352858152,
      "grad_norm": 11.347228050231934,
      "learning_rate": 8.139261350270524e-06,
      "loss": 2.8229,
      "step": 3560
    },
    {
      "epoch": 2.519407198306281,
      "grad_norm": 10.582426071166992,
      "learning_rate": 8.021641966596095e-06,
      "loss": 2.4847,
      "step": 3570
    },
    {
      "epoch": 2.5264643613267466,
      "grad_norm": 9.793582916259766,
      "learning_rate": 7.904022582921667e-06,
      "loss": 2.7888,
      "step": 3580
    },
    {
      "epoch": 2.5335215243472122,
      "grad_norm": 10.542313575744629,
      "learning_rate": 7.786403199247235e-06,
      "loss": 2.6154,
      "step": 3590
    },
    {
      "epoch": 2.5405786873676783,
      "grad_norm": 10.273268699645996,
      "learning_rate": 7.668783815572807e-06,
      "loss": 2.5743,
      "step": 3600
    },
    {
      "epoch": 2.547635850388144,
      "grad_norm": 10.684529304504395,
      "learning_rate": 7.551164431898378e-06,
      "loss": 2.5346,
      "step": 3610
    },
    {
      "epoch": 2.5546930134086097,
      "grad_norm": 11.952486991882324,
      "learning_rate": 7.433545048223947e-06,
      "loss": 2.9904,
      "step": 3620
    },
    {
      "epoch": 2.5617501764290758,
      "grad_norm": 9.280716896057129,
      "learning_rate": 7.315925664549518e-06,
      "loss": 1.8246,
      "step": 3630
    },
    {
      "epoch": 2.5688073394495414,
      "grad_norm": 9.353923797607422,
      "learning_rate": 7.198306280875089e-06,
      "loss": 2.7214,
      "step": 3640
    },
    {
      "epoch": 2.575864502470007,
      "grad_norm": 11.44091510772705,
      "learning_rate": 7.080686897200659e-06,
      "loss": 2.5165,
      "step": 3650
    },
    {
      "epoch": 2.5829216654904728,
      "grad_norm": 10.520855903625488,
      "learning_rate": 6.963067513526229e-06,
      "loss": 2.9014,
      "step": 3660
    },
    {
      "epoch": 2.5899788285109384,
      "grad_norm": 9.215030670166016,
      "learning_rate": 6.8454481298518e-06,
      "loss": 2.7515,
      "step": 3670
    },
    {
      "epoch": 2.5970359915314045,
      "grad_norm": 9.737756729125977,
      "learning_rate": 6.72782874617737e-06,
      "loss": 2.4385,
      "step": 3680
    },
    {
      "epoch": 2.60409315455187,
      "grad_norm": 7.986388683319092,
      "learning_rate": 6.610209362502941e-06,
      "loss": 2.6973,
      "step": 3690
    },
    {
      "epoch": 2.611150317572336,
      "grad_norm": 9.15822696685791,
      "learning_rate": 6.492589978828512e-06,
      "loss": 1.9051,
      "step": 3700
    },
    {
      "epoch": 2.618207480592802,
      "grad_norm": 9.523468017578125,
      "learning_rate": 6.374970595154081e-06,
      "loss": 2.4072,
      "step": 3710
    },
    {
      "epoch": 2.6252646436132676,
      "grad_norm": 7.9203925132751465,
      "learning_rate": 6.257351211479652e-06,
      "loss": 2.5571,
      "step": 3720
    },
    {
      "epoch": 2.6323218066337333,
      "grad_norm": 8.790275573730469,
      "learning_rate": 6.139731827805223e-06,
      "loss": 2.2495,
      "step": 3730
    },
    {
      "epoch": 2.639378969654199,
      "grad_norm": 11.184148788452148,
      "learning_rate": 6.022112444130793e-06,
      "loss": 2.5475,
      "step": 3740
    },
    {
      "epoch": 2.6464361326746646,
      "grad_norm": 8.287108421325684,
      "learning_rate": 5.904493060456363e-06,
      "loss": 2.6213,
      "step": 3750
    },
    {
      "epoch": 2.6534932956951307,
      "grad_norm": 9.369872093200684,
      "learning_rate": 5.7868736767819345e-06,
      "loss": 2.1317,
      "step": 3760
    },
    {
      "epoch": 2.6605504587155964,
      "grad_norm": 10.552488327026367,
      "learning_rate": 5.669254293107505e-06,
      "loss": 2.5095,
      "step": 3770
    },
    {
      "epoch": 2.667607621736062,
      "grad_norm": 8.197866439819336,
      "learning_rate": 5.551634909433075e-06,
      "loss": 2.1792,
      "step": 3780
    },
    {
      "epoch": 2.674664784756528,
      "grad_norm": 7.943900108337402,
      "learning_rate": 5.434015525758645e-06,
      "loss": 2.6609,
      "step": 3790
    },
    {
      "epoch": 2.681721947776994,
      "grad_norm": 7.183632850646973,
      "learning_rate": 5.316396142084216e-06,
      "loss": 2.2815,
      "step": 3800
    },
    {
      "epoch": 2.6887791107974595,
      "grad_norm": 11.467835426330566,
      "learning_rate": 5.198776758409786e-06,
      "loss": 2.5144,
      "step": 3810
    },
    {
      "epoch": 2.695836273817925,
      "grad_norm": 10.76943588256836,
      "learning_rate": 5.081157374735357e-06,
      "loss": 2.2196,
      "step": 3820
    },
    {
      "epoch": 2.702893436838391,
      "grad_norm": 9.137214660644531,
      "learning_rate": 4.963537991060927e-06,
      "loss": 2.4905,
      "step": 3830
    },
    {
      "epoch": 2.709950599858857,
      "grad_norm": 6.4163689613342285,
      "learning_rate": 4.845918607386498e-06,
      "loss": 2.5604,
      "step": 3840
    },
    {
      "epoch": 2.7170077628793226,
      "grad_norm": 6.68852424621582,
      "learning_rate": 4.728299223712068e-06,
      "loss": 2.4276,
      "step": 3850
    },
    {
      "epoch": 2.724064925899788,
      "grad_norm": 9.912717819213867,
      "learning_rate": 4.610679840037638e-06,
      "loss": 2.3825,
      "step": 3860
    },
    {
      "epoch": 2.7311220889202543,
      "grad_norm": 6.295912742614746,
      "learning_rate": 4.4930604563632085e-06,
      "loss": 2.2931,
      "step": 3870
    },
    {
      "epoch": 2.73817925194072,
      "grad_norm": 10.489960670471191,
      "learning_rate": 4.37544107268878e-06,
      "loss": 2.4168,
      "step": 3880
    },
    {
      "epoch": 2.7452364149611856,
      "grad_norm": 8.852295875549316,
      "learning_rate": 4.25782168901435e-06,
      "loss": 2.0423,
      "step": 3890
    },
    {
      "epoch": 2.7522935779816513,
      "grad_norm": 7.796558856964111,
      "learning_rate": 4.14020230533992e-06,
      "loss": 2.272,
      "step": 3900
    },
    {
      "epoch": 2.759350741002117,
      "grad_norm": 12.67815113067627,
      "learning_rate": 4.0225829216654905e-06,
      "loss": 2.2407,
      "step": 3910
    },
    {
      "epoch": 2.766407904022583,
      "grad_norm": 9.20928955078125,
      "learning_rate": 3.904963537991061e-06,
      "loss": 2.5078,
      "step": 3920
    },
    {
      "epoch": 2.7734650670430487,
      "grad_norm": 12.051678657531738,
      "learning_rate": 3.7873441543166315e-06,
      "loss": 2.4132,
      "step": 3930
    },
    {
      "epoch": 2.7805222300635144,
      "grad_norm": 10.764448165893555,
      "learning_rate": 3.6697247706422022e-06,
      "loss": 2.5073,
      "step": 3940
    },
    {
      "epoch": 2.7875793930839805,
      "grad_norm": 9.494702339172363,
      "learning_rate": 3.5521053869677725e-06,
      "loss": 2.5006,
      "step": 3950
    },
    {
      "epoch": 2.794636556104446,
      "grad_norm": 8.501363754272461,
      "learning_rate": 3.434486003293343e-06,
      "loss": 1.9781,
      "step": 3960
    },
    {
      "epoch": 2.801693719124912,
      "grad_norm": 10.92131233215332,
      "learning_rate": 3.3168666196189135e-06,
      "loss": 2.0092,
      "step": 3970
    },
    {
      "epoch": 2.8087508821453775,
      "grad_norm": 8.200364112854004,
      "learning_rate": 3.199247235944484e-06,
      "loss": 2.1742,
      "step": 3980
    },
    {
      "epoch": 2.815808045165843,
      "grad_norm": 8.884526252746582,
      "learning_rate": 3.081627852270054e-06,
      "loss": 2.5294,
      "step": 3990
    },
    {
      "epoch": 2.8228652081863093,
      "grad_norm": 8.015323638916016,
      "learning_rate": 2.964008468595625e-06,
      "loss": 2.2936,
      "step": 4000
    },
    {
      "epoch": 2.829922371206775,
      "grad_norm": 10.627304077148438,
      "learning_rate": 2.846389084921195e-06,
      "loss": 2.6416,
      "step": 4010
    },
    {
      "epoch": 2.8369795342272406,
      "grad_norm": 11.26171875,
      "learning_rate": 2.728769701246766e-06,
      "loss": 2.4617,
      "step": 4020
    },
    {
      "epoch": 2.8440366972477067,
      "grad_norm": 8.817094802856445,
      "learning_rate": 2.611150317572336e-06,
      "loss": 2.4763,
      "step": 4030
    },
    {
      "epoch": 2.8510938602681724,
      "grad_norm": 9.963401794433594,
      "learning_rate": 2.4935309338979064e-06,
      "loss": 2.4596,
      "step": 4040
    },
    {
      "epoch": 2.858151023288638,
      "grad_norm": 9.966653823852539,
      "learning_rate": 2.375911550223477e-06,
      "loss": 2.5283,
      "step": 4050
    },
    {
      "epoch": 2.8652081863091037,
      "grad_norm": 10.425721168518066,
      "learning_rate": 2.2582921665490474e-06,
      "loss": 2.1561,
      "step": 4060
    },
    {
      "epoch": 2.8722653493295693,
      "grad_norm": 6.981555938720703,
      "learning_rate": 2.1406727828746177e-06,
      "loss": 2.0915,
      "step": 4070
    },
    {
      "epoch": 2.8793225123500354,
      "grad_norm": 8.316436767578125,
      "learning_rate": 2.0230533992001884e-06,
      "loss": 2.2133,
      "step": 4080
    },
    {
      "epoch": 2.886379675370501,
      "grad_norm": 10.55367374420166,
      "learning_rate": 1.905434015525759e-06,
      "loss": 2.4383,
      "step": 4090
    },
    {
      "epoch": 2.8934368383909668,
      "grad_norm": 7.857081890106201,
      "learning_rate": 1.787814631851329e-06,
      "loss": 2.1681,
      "step": 4100
    },
    {
      "epoch": 2.900494001411433,
      "grad_norm": 10.251871109008789,
      "learning_rate": 1.6701952481768997e-06,
      "loss": 2.5266,
      "step": 4110
    },
    {
      "epoch": 2.9075511644318985,
      "grad_norm": 11.922283172607422,
      "learning_rate": 1.5525758645024702e-06,
      "loss": 2.8465,
      "step": 4120
    },
    {
      "epoch": 2.914608327452364,
      "grad_norm": 12.421183586120605,
      "learning_rate": 1.4349564808280405e-06,
      "loss": 1.7195,
      "step": 4130
    },
    {
      "epoch": 2.92166549047283,
      "grad_norm": 8.6067476272583,
      "learning_rate": 1.317337097153611e-06,
      "loss": 2.344,
      "step": 4140
    },
    {
      "epoch": 2.9287226534932955,
      "grad_norm": 6.5121541023254395,
      "learning_rate": 1.1997177134791815e-06,
      "loss": 2.2108,
      "step": 4150
    },
    {
      "epoch": 2.9357798165137616,
      "grad_norm": 10.018781661987305,
      "learning_rate": 1.082098329804752e-06,
      "loss": 2.1946,
      "step": 4160
    },
    {
      "epoch": 2.9428369795342273,
      "grad_norm": 9.195708274841309,
      "learning_rate": 9.644789461303223e-07,
      "loss": 2.4411,
      "step": 4170
    },
    {
      "epoch": 2.949894142554693,
      "grad_norm": 7.995297431945801,
      "learning_rate": 8.468595624558928e-07,
      "loss": 2.128,
      "step": 4180
    },
    {
      "epoch": 2.956951305575159,
      "grad_norm": 10.604729652404785,
      "learning_rate": 7.292401787814632e-07,
      "loss": 2.1438,
      "step": 4190
    },
    {
      "epoch": 2.9640084685956247,
      "grad_norm": 9.638379096984863,
      "learning_rate": 6.116207951070337e-07,
      "loss": 2.5639,
      "step": 4200
    },
    {
      "epoch": 2.9710656316160904,
      "grad_norm": 9.024795532226562,
      "learning_rate": 4.940014114326042e-07,
      "loss": 2.4359,
      "step": 4210
    },
    {
      "epoch": 2.978122794636556,
      "grad_norm": 8.860577583312988,
      "learning_rate": 3.7638202775817457e-07,
      "loss": 2.0047,
      "step": 4220
    },
    {
      "epoch": 2.9851799576570217,
      "grad_norm": 10.630794525146484,
      "learning_rate": 2.58762644083745e-07,
      "loss": 2.4377,
      "step": 4230
    },
    {
      "epoch": 2.992237120677488,
      "grad_norm": 9.503963470458984,
      "learning_rate": 1.4114326040931546e-07,
      "loss": 2.6381,
      "step": 4240
    },
    {
      "epoch": 2.9992942836979535,
      "grad_norm": 10.410962104797363,
      "learning_rate": 2.352387673488591e-08,
      "loss": 2.3361,
      "step": 4250
    }
  ],
  "logging_steps": 10,
  "max_steps": 4251,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 277688107008000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
